{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "48\n",
      "20\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "def clean_data(fi, fo, header, suffix):\n",
    "    head = fi.readline().strip(\"\\n\").split(\",\")\n",
    "    head = [h.strip('\"') for h in head]\n",
    "    for i, h in enumerate(head):\n",
    "        if h == \"nomprov\":\n",
    "            ip = i\n",
    "    print(ip)\n",
    "    n = len(head)\n",
    "    if header:\n",
    "        fo.write(\"%s\\n\" % \",\".join(head))\n",
    "\n",
    "    print(n)\n",
    "    for line in fi:\n",
    "        fields = line.strip(\"\\n\").split(\",\")\n",
    "        # this process is done because there are elements of 'nomprov' conatining comma\n",
    "        if len(fields) > n:\n",
    "            prov = fields[ip] + fields[ip+1]\n",
    "            del fields[ip]\n",
    "            fields[ip] = prov\n",
    "        assert len(fields) == n\n",
    "        fields = [field.strip() for field in fields]\n",
    "        fo.write(\"%s%s\\n\" % (\",\".join(fields), suffix))\n",
    "\n",
    "with open(\"data/8th.clean.all.csv\", \"w\") as f:\n",
    "    clean_data(open(\"data/train_ver2.csv\"), f, True, \"\")\n",
    "    comma24 = \"\".join([\",\" for i in range(24)])\n",
    "    clean_data(open(\"data/test_ver2.csv\"), f, False, comma24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load train csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/irteam/.pyenv/versions/3.6.4/envs/venv/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2910: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load train csv: cpu 63.94, time 63.94\n",
      "\n",
      "fill products NA...\n",
      "fill products NA: cpu 37.97, time 37.98\n",
      "\n",
      "apply transforms...\n",
      "assert_uniq renta_top (array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
      "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
      "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
      "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
      "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
      "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
      "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100], dtype=int8), array([11487211,  3022340,     5936,     1854,     1584,     1495,\n",
      "           1444,     1416,     1256,     1218,     1114,     1076,\n",
      "           1017,      952,      941,      899,      855,      844,\n",
      "            823,      813,      756,      723,      718,      712,\n",
      "            694,      690,      666,      663,      661,      657,\n",
      "            645,      624,      619,      618,      602,      595,\n",
      "            594,      593,      590,      588,      577,      576,\n",
      "            573,      572,      570,      564,      557,      549,\n",
      "            546,      546,      542,      536,      532,      527,\n",
      "            521,      521,      520,      517,      505,      505,\n",
      "            504,      499,      498,      486,      480,      477,\n",
      "            467,      464,      462,      461,      460,      458,\n",
      "            456,      456,      455,      450,      445,      444,\n",
      "            439,      438,      435,      432,      432,      426,\n",
      "            426,      421,      420,      420,      415,      413,\n",
      "            413,      410,      408,      404,      402,      402,\n",
      "            401,      400,      399,      398,      396]))\n",
      "indresi indresi_n\n",
      "indext indext_s\n",
      "conyuemp conyuemp_n\n",
      "sexo sexo_h\n",
      "sexo sexo_v\n",
      "ind_empleado ind_empleado_a\n",
      "ind_empleado ind_empleado_b\n",
      "ind_empleado ind_empleado_f\n",
      "ind_empleado ind_empleado_n\n",
      "ind_nuevo ind_nuevo_new\n",
      "segmento segmento_top\n",
      "segmento segmento_particulares\n",
      "segmento segmento_universitario\n",
      "indfall indfall_s\n",
      "indrel indrel_1\n",
      "indrel indrel_99\n",
      "assert_uniq indrel_1mes (array([0, 1, 2, 3, 4, 5], dtype=int8), array([  149804, 14420246,     1317,     4377,      306,      874]))\n",
      "tiprel_1mes tiprel_1mes_a\n",
      "assert_uniq tiprel_1mes_a (array([0, 1], dtype=int8), array([7996179, 6580745]))\n",
      "tiprel_1mes tiprel_1mes_i\n",
      "assert_uniq tiprel_1mes_i (array([0, 1], dtype=int8), array([6736106, 7840818]))\n",
      "tiprel_1mes tiprel_1mes_p\n",
      "assert_uniq tiprel_1mes_p (array([0, 1], dtype=int8), array([14572241,     4683]))\n",
      "tiprel_1mes tiprel_1mes_r\n",
      "assert_uniq tiprel_1mes_r (array([0, 1], dtype=int8), array([14576054,      870]))\n",
      "apply transforms: cpu 343.64, time 343.62\n",
      "\n",
      "make prev1 DF...\n",
      "make prev1 DF: cpu 6.37, time 6.38\n",
      "\n",
      "make prev2 DF...\n",
      "make prev2 DF: cpu 6.28, time 6.28\n",
      "\n",
      "make prev3 DF...\n",
      "make prev3 DF: cpu 6.33, time 6.32\n",
      "\n",
      "make prev4 DF...\n",
      "make prev4 DF: cpu 6.41, time 6.41\n",
      "\n",
      "make prev5 DF...\n",
      "make prev5 DF: cpu 6.31, time 6.31\n",
      "\n",
      "join train with prev1...\n",
      "join inner...\n",
      "before join 14576924\n",
      "after join 13612036\n",
      "join inner: cpu 42.55, time 42.55\n",
      "\n",
      "join train with prev1: cpu 43.20, time 43.20\n",
      "\n",
      "join train with prev2...\n",
      "join left...\n",
      "before join 13612036\n",
      "after join 13612036\n",
      "join left: cpu 44.57, time 44.57\n",
      "\n",
      "join train with prev2: cpu 45.11, time 45.12\n",
      "\n",
      "join train with prev3...\n",
      "join left...\n",
      "before join 13612036\n",
      "after join 13612036\n",
      "join left: cpu 47.88, time 47.86\n",
      "\n",
      "join train with prev3: cpu 48.52, time 48.51\n",
      "\n",
      "join train with prev4...\n",
      "join left...\n",
      "before join 13612036\n",
      "after join 13612036\n",
      "join left: cpu 49.51, time 49.50\n",
      "\n",
      "join train with prev4: cpu 50.17, time 50.17\n",
      "\n",
      "join train with prev5...\n",
      "join left...\n",
      "before join 13612036\n",
      "after join 13612036\n",
      "join left: cpu 52.28, time 52.28\n",
      "\n",
      "join train with prev5: cpu 52.78, time 52.79\n",
      "\n",
      "\n",
      "ind_ahor_fin_ult1\n",
      "['ind_ahor_fin_ult1_prev1', 'ind_ahor_fin_ult1_prev2', 'ind_ahor_fin_ult1_prev3']\n",
      "['ind_ahor_fin_ult1_prev1', 'ind_ahor_fin_ult1_prev2', 'ind_ahor_fin_ult1_prev3', 'ind_ahor_fin_ult1_prev4', 'ind_ahor_fin_ult1_prev5']\n",
      "['ind_ahor_fin_ult1_prev2', 'ind_ahor_fin_ult1_prev3', 'ind_ahor_fin_ult1_prev4', 'ind_ahor_fin_ult1_prev5']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/irteam/.pyenv/versions/3.6.4/envs/venv/lib/python3.6/site-packages/numpy/lib/nanfunctions.py:1427: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  keepdims=keepdims)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ind_aval_fin_ult1\n",
      "['ind_aval_fin_ult1_prev1', 'ind_aval_fin_ult1_prev2', 'ind_aval_fin_ult1_prev3']\n",
      "['ind_aval_fin_ult1_prev1', 'ind_aval_fin_ult1_prev2', 'ind_aval_fin_ult1_prev3', 'ind_aval_fin_ult1_prev4', 'ind_aval_fin_ult1_prev5']\n",
      "['ind_aval_fin_ult1_prev2', 'ind_aval_fin_ult1_prev3', 'ind_aval_fin_ult1_prev4', 'ind_aval_fin_ult1_prev5']\n",
      "\n",
      "ind_cco_fin_ult1\n",
      "['ind_cco_fin_ult1_prev1', 'ind_cco_fin_ult1_prev2', 'ind_cco_fin_ult1_prev3']\n",
      "['ind_cco_fin_ult1_prev1', 'ind_cco_fin_ult1_prev2', 'ind_cco_fin_ult1_prev3', 'ind_cco_fin_ult1_prev4', 'ind_cco_fin_ult1_prev5']\n",
      "['ind_cco_fin_ult1_prev2', 'ind_cco_fin_ult1_prev3', 'ind_cco_fin_ult1_prev4', 'ind_cco_fin_ult1_prev5']\n",
      "\n",
      "ind_cder_fin_ult1\n",
      "['ind_cder_fin_ult1_prev1', 'ind_cder_fin_ult1_prev2', 'ind_cder_fin_ult1_prev3']\n",
      "['ind_cder_fin_ult1_prev1', 'ind_cder_fin_ult1_prev2', 'ind_cder_fin_ult1_prev3', 'ind_cder_fin_ult1_prev4', 'ind_cder_fin_ult1_prev5']\n",
      "['ind_cder_fin_ult1_prev2', 'ind_cder_fin_ult1_prev3', 'ind_cder_fin_ult1_prev4', 'ind_cder_fin_ult1_prev5']\n",
      "\n",
      "ind_cno_fin_ult1\n",
      "['ind_cno_fin_ult1_prev1', 'ind_cno_fin_ult1_prev2', 'ind_cno_fin_ult1_prev3']\n",
      "['ind_cno_fin_ult1_prev1', 'ind_cno_fin_ult1_prev2', 'ind_cno_fin_ult1_prev3', 'ind_cno_fin_ult1_prev4', 'ind_cno_fin_ult1_prev5']\n",
      "['ind_cno_fin_ult1_prev2', 'ind_cno_fin_ult1_prev3', 'ind_cno_fin_ult1_prev4', 'ind_cno_fin_ult1_prev5']\n",
      "\n",
      "ind_ctju_fin_ult1\n",
      "['ind_ctju_fin_ult1_prev1', 'ind_ctju_fin_ult1_prev2', 'ind_ctju_fin_ult1_prev3']\n",
      "['ind_ctju_fin_ult1_prev1', 'ind_ctju_fin_ult1_prev2', 'ind_ctju_fin_ult1_prev3', 'ind_ctju_fin_ult1_prev4', 'ind_ctju_fin_ult1_prev5']\n",
      "['ind_ctju_fin_ult1_prev2', 'ind_ctju_fin_ult1_prev3', 'ind_ctju_fin_ult1_prev4', 'ind_ctju_fin_ult1_prev5']\n",
      "\n",
      "ind_ctma_fin_ult1\n",
      "['ind_ctma_fin_ult1_prev1', 'ind_ctma_fin_ult1_prev2', 'ind_ctma_fin_ult1_prev3']\n",
      "['ind_ctma_fin_ult1_prev1', 'ind_ctma_fin_ult1_prev2', 'ind_ctma_fin_ult1_prev3', 'ind_ctma_fin_ult1_prev4', 'ind_ctma_fin_ult1_prev5']\n",
      "['ind_ctma_fin_ult1_prev2', 'ind_ctma_fin_ult1_prev3', 'ind_ctma_fin_ult1_prev4', 'ind_ctma_fin_ult1_prev5']\n",
      "\n",
      "ind_ctop_fin_ult1\n",
      "['ind_ctop_fin_ult1_prev1', 'ind_ctop_fin_ult1_prev2', 'ind_ctop_fin_ult1_prev3']\n",
      "['ind_ctop_fin_ult1_prev1', 'ind_ctop_fin_ult1_prev2', 'ind_ctop_fin_ult1_prev3', 'ind_ctop_fin_ult1_prev4', 'ind_ctop_fin_ult1_prev5']\n",
      "['ind_ctop_fin_ult1_prev2', 'ind_ctop_fin_ult1_prev3', 'ind_ctop_fin_ult1_prev4', 'ind_ctop_fin_ult1_prev5']\n",
      "\n",
      "ind_ctpp_fin_ult1\n",
      "['ind_ctpp_fin_ult1_prev1', 'ind_ctpp_fin_ult1_prev2', 'ind_ctpp_fin_ult1_prev3']\n",
      "['ind_ctpp_fin_ult1_prev1', 'ind_ctpp_fin_ult1_prev2', 'ind_ctpp_fin_ult1_prev3', 'ind_ctpp_fin_ult1_prev4', 'ind_ctpp_fin_ult1_prev5']\n",
      "['ind_ctpp_fin_ult1_prev2', 'ind_ctpp_fin_ult1_prev3', 'ind_ctpp_fin_ult1_prev4', 'ind_ctpp_fin_ult1_prev5']\n",
      "\n",
      "ind_deco_fin_ult1\n",
      "['ind_deco_fin_ult1_prev1', 'ind_deco_fin_ult1_prev2', 'ind_deco_fin_ult1_prev3']\n",
      "['ind_deco_fin_ult1_prev1', 'ind_deco_fin_ult1_prev2', 'ind_deco_fin_ult1_prev3', 'ind_deco_fin_ult1_prev4', 'ind_deco_fin_ult1_prev5']\n",
      "['ind_deco_fin_ult1_prev2', 'ind_deco_fin_ult1_prev3', 'ind_deco_fin_ult1_prev4', 'ind_deco_fin_ult1_prev5']\n",
      "\n",
      "ind_deme_fin_ult1\n",
      "['ind_deme_fin_ult1_prev1', 'ind_deme_fin_ult1_prev2', 'ind_deme_fin_ult1_prev3']\n",
      "['ind_deme_fin_ult1_prev1', 'ind_deme_fin_ult1_prev2', 'ind_deme_fin_ult1_prev3', 'ind_deme_fin_ult1_prev4', 'ind_deme_fin_ult1_prev5']\n",
      "['ind_deme_fin_ult1_prev2', 'ind_deme_fin_ult1_prev3', 'ind_deme_fin_ult1_prev4', 'ind_deme_fin_ult1_prev5']\n",
      "\n",
      "ind_dela_fin_ult1\n",
      "['ind_dela_fin_ult1_prev1', 'ind_dela_fin_ult1_prev2', 'ind_dela_fin_ult1_prev3']\n",
      "['ind_dela_fin_ult1_prev1', 'ind_dela_fin_ult1_prev2', 'ind_dela_fin_ult1_prev3', 'ind_dela_fin_ult1_prev4', 'ind_dela_fin_ult1_prev5']\n",
      "['ind_dela_fin_ult1_prev2', 'ind_dela_fin_ult1_prev3', 'ind_dela_fin_ult1_prev4', 'ind_dela_fin_ult1_prev5']\n",
      "\n",
      "ind_ecue_fin_ult1\n",
      "['ind_ecue_fin_ult1_prev1', 'ind_ecue_fin_ult1_prev2', 'ind_ecue_fin_ult1_prev3']\n",
      "['ind_ecue_fin_ult1_prev1', 'ind_ecue_fin_ult1_prev2', 'ind_ecue_fin_ult1_prev3', 'ind_ecue_fin_ult1_prev4', 'ind_ecue_fin_ult1_prev5']\n",
      "['ind_ecue_fin_ult1_prev2', 'ind_ecue_fin_ult1_prev3', 'ind_ecue_fin_ult1_prev4', 'ind_ecue_fin_ult1_prev5']\n",
      "\n",
      "ind_fond_fin_ult1\n",
      "['ind_fond_fin_ult1_prev1', 'ind_fond_fin_ult1_prev2', 'ind_fond_fin_ult1_prev3']\n",
      "['ind_fond_fin_ult1_prev1', 'ind_fond_fin_ult1_prev2', 'ind_fond_fin_ult1_prev3', 'ind_fond_fin_ult1_prev4', 'ind_fond_fin_ult1_prev5']\n",
      "['ind_fond_fin_ult1_prev2', 'ind_fond_fin_ult1_prev3', 'ind_fond_fin_ult1_prev4', 'ind_fond_fin_ult1_prev5']\n",
      "\n",
      "ind_hip_fin_ult1\n",
      "['ind_hip_fin_ult1_prev1', 'ind_hip_fin_ult1_prev2', 'ind_hip_fin_ult1_prev3']\n",
      "['ind_hip_fin_ult1_prev1', 'ind_hip_fin_ult1_prev2', 'ind_hip_fin_ult1_prev3', 'ind_hip_fin_ult1_prev4', 'ind_hip_fin_ult1_prev5']\n",
      "['ind_hip_fin_ult1_prev2', 'ind_hip_fin_ult1_prev3', 'ind_hip_fin_ult1_prev4', 'ind_hip_fin_ult1_prev5']\n",
      "\n",
      "ind_plan_fin_ult1\n",
      "['ind_plan_fin_ult1_prev1', 'ind_plan_fin_ult1_prev2', 'ind_plan_fin_ult1_prev3']\n",
      "['ind_plan_fin_ult1_prev1', 'ind_plan_fin_ult1_prev2', 'ind_plan_fin_ult1_prev3', 'ind_plan_fin_ult1_prev4', 'ind_plan_fin_ult1_prev5']\n",
      "['ind_plan_fin_ult1_prev2', 'ind_plan_fin_ult1_prev3', 'ind_plan_fin_ult1_prev4', 'ind_plan_fin_ult1_prev5']\n",
      "\n",
      "ind_pres_fin_ult1\n",
      "['ind_pres_fin_ult1_prev1', 'ind_pres_fin_ult1_prev2', 'ind_pres_fin_ult1_prev3']\n",
      "['ind_pres_fin_ult1_prev1', 'ind_pres_fin_ult1_prev2', 'ind_pres_fin_ult1_prev3', 'ind_pres_fin_ult1_prev4', 'ind_pres_fin_ult1_prev5']\n",
      "['ind_pres_fin_ult1_prev2', 'ind_pres_fin_ult1_prev3', 'ind_pres_fin_ult1_prev4', 'ind_pres_fin_ult1_prev5']\n",
      "\n",
      "ind_reca_fin_ult1\n",
      "['ind_reca_fin_ult1_prev1', 'ind_reca_fin_ult1_prev2', 'ind_reca_fin_ult1_prev3']\n",
      "['ind_reca_fin_ult1_prev1', 'ind_reca_fin_ult1_prev2', 'ind_reca_fin_ult1_prev3', 'ind_reca_fin_ult1_prev4', 'ind_reca_fin_ult1_prev5']\n",
      "['ind_reca_fin_ult1_prev2', 'ind_reca_fin_ult1_prev3', 'ind_reca_fin_ult1_prev4', 'ind_reca_fin_ult1_prev5']\n",
      "\n",
      "ind_tjcr_fin_ult1\n",
      "['ind_tjcr_fin_ult1_prev1', 'ind_tjcr_fin_ult1_prev2', 'ind_tjcr_fin_ult1_prev3']\n",
      "['ind_tjcr_fin_ult1_prev1', 'ind_tjcr_fin_ult1_prev2', 'ind_tjcr_fin_ult1_prev3', 'ind_tjcr_fin_ult1_prev4', 'ind_tjcr_fin_ult1_prev5']\n",
      "['ind_tjcr_fin_ult1_prev2', 'ind_tjcr_fin_ult1_prev3', 'ind_tjcr_fin_ult1_prev4', 'ind_tjcr_fin_ult1_prev5']\n",
      "\n",
      "ind_valo_fin_ult1\n",
      "['ind_valo_fin_ult1_prev1', 'ind_valo_fin_ult1_prev2', 'ind_valo_fin_ult1_prev3']\n",
      "['ind_valo_fin_ult1_prev1', 'ind_valo_fin_ult1_prev2', 'ind_valo_fin_ult1_prev3', 'ind_valo_fin_ult1_prev4', 'ind_valo_fin_ult1_prev5']\n",
      "['ind_valo_fin_ult1_prev2', 'ind_valo_fin_ult1_prev3', 'ind_valo_fin_ult1_prev4', 'ind_valo_fin_ult1_prev5']\n",
      "\n",
      "ind_viv_fin_ult1\n",
      "['ind_viv_fin_ult1_prev1', 'ind_viv_fin_ult1_prev2', 'ind_viv_fin_ult1_prev3']\n",
      "['ind_viv_fin_ult1_prev1', 'ind_viv_fin_ult1_prev2', 'ind_viv_fin_ult1_prev3', 'ind_viv_fin_ult1_prev4', 'ind_viv_fin_ult1_prev5']\n",
      "['ind_viv_fin_ult1_prev2', 'ind_viv_fin_ult1_prev3', 'ind_viv_fin_ult1_prev4', 'ind_viv_fin_ult1_prev5']\n",
      "\n",
      "ind_nomina_ult1\n",
      "['ind_nomina_ult1_prev1', 'ind_nomina_ult1_prev2', 'ind_nomina_ult1_prev3']\n",
      "['ind_nomina_ult1_prev1', 'ind_nomina_ult1_prev2', 'ind_nomina_ult1_prev3', 'ind_nomina_ult1_prev4', 'ind_nomina_ult1_prev5']\n",
      "['ind_nomina_ult1_prev2', 'ind_nomina_ult1_prev3', 'ind_nomina_ult1_prev4', 'ind_nomina_ult1_prev5']\n",
      "\n",
      "ind_nom_pens_ult1\n",
      "['ind_nom_pens_ult1_prev1', 'ind_nom_pens_ult1_prev2', 'ind_nom_pens_ult1_prev3']\n",
      "['ind_nom_pens_ult1_prev1', 'ind_nom_pens_ult1_prev2', 'ind_nom_pens_ult1_prev3', 'ind_nom_pens_ult1_prev4', 'ind_nom_pens_ult1_prev5']\n",
      "['ind_nom_pens_ult1_prev2', 'ind_nom_pens_ult1_prev3', 'ind_nom_pens_ult1_prev4', 'ind_nom_pens_ult1_prev5']\n",
      "\n",
      "ind_recibo_ult1\n",
      "['ind_recibo_ult1_prev1', 'ind_recibo_ult1_prev2', 'ind_recibo_ult1_prev3']\n",
      "['ind_recibo_ult1_prev1', 'ind_recibo_ult1_prev2', 'ind_recibo_ult1_prev3', 'ind_recibo_ult1_prev4', 'ind_recibo_ult1_prev5']\n",
      "['ind_recibo_ult1_prev2', 'ind_recibo_ult1_prev3', 'ind_recibo_ult1_prev4', 'ind_recibo_ult1_prev5']\n",
      "\n",
      "ind_ahor_fin_ult1\n",
      "['ind_ahor_fin_ult1_prev2', 'ind_ahor_fin_ult1_prev3']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/irteam/.pyenv/versions/3.6.4/envs/venv/lib/python3.6/site-packages/ipykernel_launcher.py:241: RuntimeWarning: All-NaN axis encountered\n",
      "/home1/irteam/.pyenv/versions/3.6.4/envs/venv/lib/python3.6/site-packages/ipykernel_launcher.py:244: RuntimeWarning: All-NaN slice encountered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ind_ahor_fin_ult1_prev2', 'ind_ahor_fin_ult1_prev3', 'ind_ahor_fin_ult1_prev4', 'ind_ahor_fin_ult1_prev5']\n",
      "\n",
      "ind_aval_fin_ult1\n",
      "['ind_aval_fin_ult1_prev2', 'ind_aval_fin_ult1_prev3']\n",
      "['ind_aval_fin_ult1_prev2', 'ind_aval_fin_ult1_prev3', 'ind_aval_fin_ult1_prev4', 'ind_aval_fin_ult1_prev5']\n",
      "\n",
      "ind_cco_fin_ult1\n",
      "['ind_cco_fin_ult1_prev2', 'ind_cco_fin_ult1_prev3']\n",
      "['ind_cco_fin_ult1_prev2', 'ind_cco_fin_ult1_prev3', 'ind_cco_fin_ult1_prev4', 'ind_cco_fin_ult1_prev5']\n",
      "\n",
      "ind_cder_fin_ult1\n",
      "['ind_cder_fin_ult1_prev2', 'ind_cder_fin_ult1_prev3']\n",
      "['ind_cder_fin_ult1_prev2', 'ind_cder_fin_ult1_prev3', 'ind_cder_fin_ult1_prev4', 'ind_cder_fin_ult1_prev5']\n",
      "\n",
      "ind_cno_fin_ult1\n",
      "['ind_cno_fin_ult1_prev2', 'ind_cno_fin_ult1_prev3']\n",
      "['ind_cno_fin_ult1_prev2', 'ind_cno_fin_ult1_prev3', 'ind_cno_fin_ult1_prev4', 'ind_cno_fin_ult1_prev5']\n",
      "\n",
      "ind_ctju_fin_ult1\n",
      "['ind_ctju_fin_ult1_prev2', 'ind_ctju_fin_ult1_prev3']\n",
      "['ind_ctju_fin_ult1_prev2', 'ind_ctju_fin_ult1_prev3', 'ind_ctju_fin_ult1_prev4', 'ind_ctju_fin_ult1_prev5']\n",
      "\n",
      "ind_ctma_fin_ult1\n",
      "['ind_ctma_fin_ult1_prev2', 'ind_ctma_fin_ult1_prev3']\n",
      "['ind_ctma_fin_ult1_prev2', 'ind_ctma_fin_ult1_prev3', 'ind_ctma_fin_ult1_prev4', 'ind_ctma_fin_ult1_prev5']\n",
      "\n",
      "ind_ctop_fin_ult1\n",
      "['ind_ctop_fin_ult1_prev2', 'ind_ctop_fin_ult1_prev3']\n",
      "['ind_ctop_fin_ult1_prev2', 'ind_ctop_fin_ult1_prev3', 'ind_ctop_fin_ult1_prev4', 'ind_ctop_fin_ult1_prev5']\n",
      "\n",
      "ind_ctpp_fin_ult1\n",
      "['ind_ctpp_fin_ult1_prev2', 'ind_ctpp_fin_ult1_prev3']\n",
      "['ind_ctpp_fin_ult1_prev2', 'ind_ctpp_fin_ult1_prev3', 'ind_ctpp_fin_ult1_prev4', 'ind_ctpp_fin_ult1_prev5']\n",
      "\n",
      "ind_deco_fin_ult1\n",
      "['ind_deco_fin_ult1_prev2', 'ind_deco_fin_ult1_prev3']\n",
      "['ind_deco_fin_ult1_prev2', 'ind_deco_fin_ult1_prev3', 'ind_deco_fin_ult1_prev4', 'ind_deco_fin_ult1_prev5']\n",
      "\n",
      "ind_deme_fin_ult1\n",
      "['ind_deme_fin_ult1_prev2', 'ind_deme_fin_ult1_prev3']\n",
      "['ind_deme_fin_ult1_prev2', 'ind_deme_fin_ult1_prev3', 'ind_deme_fin_ult1_prev4', 'ind_deme_fin_ult1_prev5']\n",
      "\n",
      "ind_dela_fin_ult1\n",
      "['ind_dela_fin_ult1_prev2', 'ind_dela_fin_ult1_prev3']\n",
      "['ind_dela_fin_ult1_prev2', 'ind_dela_fin_ult1_prev3', 'ind_dela_fin_ult1_prev4', 'ind_dela_fin_ult1_prev5']\n",
      "\n",
      "ind_ecue_fin_ult1\n",
      "['ind_ecue_fin_ult1_prev2', 'ind_ecue_fin_ult1_prev3']\n",
      "['ind_ecue_fin_ult1_prev2', 'ind_ecue_fin_ult1_prev3', 'ind_ecue_fin_ult1_prev4', 'ind_ecue_fin_ult1_prev5']\n",
      "\n",
      "ind_fond_fin_ult1\n",
      "['ind_fond_fin_ult1_prev2', 'ind_fond_fin_ult1_prev3']\n",
      "['ind_fond_fin_ult1_prev2', 'ind_fond_fin_ult1_prev3', 'ind_fond_fin_ult1_prev4', 'ind_fond_fin_ult1_prev5']\n",
      "\n",
      "ind_hip_fin_ult1\n",
      "['ind_hip_fin_ult1_prev2', 'ind_hip_fin_ult1_prev3']\n",
      "['ind_hip_fin_ult1_prev2', 'ind_hip_fin_ult1_prev3', 'ind_hip_fin_ult1_prev4', 'ind_hip_fin_ult1_prev5']\n",
      "\n",
      "ind_plan_fin_ult1\n",
      "['ind_plan_fin_ult1_prev2', 'ind_plan_fin_ult1_prev3']\n",
      "['ind_plan_fin_ult1_prev2', 'ind_plan_fin_ult1_prev3', 'ind_plan_fin_ult1_prev4', 'ind_plan_fin_ult1_prev5']\n",
      "\n",
      "ind_pres_fin_ult1\n",
      "['ind_pres_fin_ult1_prev2', 'ind_pres_fin_ult1_prev3']\n",
      "['ind_pres_fin_ult1_prev2', 'ind_pres_fin_ult1_prev3', 'ind_pres_fin_ult1_prev4', 'ind_pres_fin_ult1_prev5']\n",
      "\n",
      "ind_reca_fin_ult1\n",
      "['ind_reca_fin_ult1_prev2', 'ind_reca_fin_ult1_prev3']\n",
      "['ind_reca_fin_ult1_prev2', 'ind_reca_fin_ult1_prev3', 'ind_reca_fin_ult1_prev4', 'ind_reca_fin_ult1_prev5']\n",
      "\n",
      "ind_tjcr_fin_ult1\n",
      "['ind_tjcr_fin_ult1_prev2', 'ind_tjcr_fin_ult1_prev3']\n",
      "['ind_tjcr_fin_ult1_prev2', 'ind_tjcr_fin_ult1_prev3', 'ind_tjcr_fin_ult1_prev4', 'ind_tjcr_fin_ult1_prev5']\n",
      "\n",
      "ind_valo_fin_ult1\n",
      "['ind_valo_fin_ult1_prev2', 'ind_valo_fin_ult1_prev3']\n",
      "['ind_valo_fin_ult1_prev2', 'ind_valo_fin_ult1_prev3', 'ind_valo_fin_ult1_prev4', 'ind_valo_fin_ult1_prev5']\n",
      "\n",
      "ind_viv_fin_ult1\n",
      "['ind_viv_fin_ult1_prev2', 'ind_viv_fin_ult1_prev3']\n",
      "['ind_viv_fin_ult1_prev2', 'ind_viv_fin_ult1_prev3', 'ind_viv_fin_ult1_prev4', 'ind_viv_fin_ult1_prev5']\n",
      "\n",
      "ind_nomina_ult1\n",
      "['ind_nomina_ult1_prev2', 'ind_nomina_ult1_prev3']\n",
      "['ind_nomina_ult1_prev2', 'ind_nomina_ult1_prev3', 'ind_nomina_ult1_prev4', 'ind_nomina_ult1_prev5']\n",
      "\n",
      "ind_nom_pens_ult1\n",
      "['ind_nom_pens_ult1_prev2', 'ind_nom_pens_ult1_prev3']\n",
      "['ind_nom_pens_ult1_prev2', 'ind_nom_pens_ult1_prev3', 'ind_nom_pens_ult1_prev4', 'ind_nom_pens_ult1_prev5']\n",
      "\n",
      "ind_recibo_ult1\n",
      "['ind_recibo_ult1_prev2', 'ind_recibo_ult1_prev3']\n",
      "['ind_recibo_ult1_prev2', 'ind_recibo_ult1_prev3', 'ind_recibo_ult1_prev4', 'ind_recibo_ult1_prev5']\n",
      "Remove unused columns...\n",
      "Remove unused columns: cpu 11.52, time 11.51\n",
      "\n",
      "save data...\n",
      "save data: cpu 12.14, time 12.13\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import math\n",
    "import pickle\n",
    "import zlib\n",
    "import io\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# import scipy.stats\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import engines\n",
    "from utils import *\n",
    "\n",
    "np.random.seed(2016)\n",
    "\n",
    "transformers = {}\n",
    "\n",
    "\n",
    "# print unique count for single series\n",
    "def assert_uniq(series, name):\n",
    "    uniq = np.unique(series, return_counts=True)\n",
    "    print(\"assert_uniq\", name, uniq)\n",
    "\n",
    "# LEARNT one hot encode only designated values in column\n",
    "def custom_one_hot(df, features, name, names, dtype=np.int8, check=False):\n",
    "    for n, val in names.items():\n",
    "        new_name = \"%s_%s\" % (name, n)\n",
    "        print(name, new_name)\n",
    "        df[new_name] = df[name].map(lambda x: 1 if x == val else 0).astype(dtype)\n",
    "\n",
    "        if check:\n",
    "            assert_uniq(df[new_name], new_name)\n",
    "        features.append(new_name)\n",
    "\n",
    "\n",
    "# preprocessor for categorical columns\n",
    "def label_encode(df, features, name):\n",
    "    df[name] = df[name].astype('str')\n",
    "    if name in transformers: # test\n",
    "        df[name] = transformers[name].transform(df[name])\n",
    "    else: # train\n",
    "        transformers[name] = LabelEncoder()\n",
    "        df[name] = transformers[name].fit_transform(df[name])\n",
    "    features.append(name)\n",
    "\n",
    "\n",
    "# extract top 100 frequent values and map it to original dataframe, out-of-rank is set to 0\n",
    "def encode_top(s, count=100, dtype=np.int8):\n",
    "    uniqs, freqs = np.unique(s, return_counts=True)\n",
    "    top = sorted(zip(uniqs,freqs), key=lambda vk: vk[1], reverse = True)[:count]\n",
    "    top_map = {uf[0]: l+1 for uf, l in zip(top, range(len(top)))}\n",
    "    return s.map(lambda x: top_map.get(x, 0)).astype(dtype) ### LEARNT use of .map(lambda x: x)\n",
    "\n",
    "\n",
    "# preprocessing of each columns\n",
    "def apply_transforms(train_df):\n",
    "    features = []\n",
    "    with Timer(\"apply transforms\"):\n",
    "        label_encode(train_df, features, \"canal_entrada\") # simple label encode\n",
    "        # label_encode(train_df, features, \"nomprov\") # use cod_prov only\n",
    "        label_encode(train_df, features, \"pais_residencia\") # simple label encode\n",
    "\n",
    "        train_df[\"age\"] = train_df[\"age\"].fillna(0.0).astype(np.int16) # simple int conversion\n",
    "        features.append(\"age\")\n",
    "\n",
    "        train_df[\"renta\"].fillna(1.0, inplace=True)\n",
    "        train_df[\"renta_top\"] = encode_top(train_df[\"renta\"]) # rank by top 100 frequency\n",
    "        assert_uniq(train_df[\"renta_top\"], \"renta_top\")\n",
    "        features.append(\"renta_top\")\n",
    "        train_df[\"renta\"] = train_df[\"renta\"].map(math.log) # log transform renta ### LEARNT direct transform\n",
    "        features.append(\"renta\")\n",
    "\n",
    "        # LEARNT how to use if/else in .map(lambda x: x)\n",
    "        train_df[\"antiguedad\"] = train_df[\"antiguedad\"].map(lambda x: 0.0 if x < 0 or math.isnan(x) else x+1.0).astype(np.int16)\n",
    "        features.append(\"antiguedad\")\n",
    "\n",
    "        train_df[\"tipodom\"] = train_df[\"tipodom\"].fillna(0.0).astype(np.int8)\n",
    "        features.append(\"tipodom\")\n",
    "\n",
    "        train_df[\"cod_prov\"] = train_df[\"cod_prov\"].fillna(0.0).astype(np.int8)\n",
    "        features.append(\"cod_prov\")\n",
    "\n",
    "        train_df[\"fecha_dato_month\"] = train_df[\"fecha_dato\"].map(lambda x: int(x.split(\"-\")[1])).astype(np.int8)\n",
    "        features.append(\"fecha_dato_month\")\n",
    "        train_df[\"fecha_dato_year\"] = train_df[\"fecha_dato\"].map(lambda x: float(x.split(\"-\")[0])).astype(np.int16)\n",
    "        features.append(\"fecha_dato_year\")\n",
    "        # LEARNT use of x.__class__ for nan in fecha_alta\n",
    "        train_df[\"fecha_alta_month\"] = train_df[\"fecha_alta\"].map(lambda x: 0.0 if x.__class__ is float else float(x.split(\"-\")[1])).astype(np.int8)\n",
    "        features.append(\"fecha_alta_month\")\n",
    "        train_df[\"fecha_alta_year\"] = train_df[\"fecha_alta\"].map(lambda x: 0.0 if x.__class__ is float else float(x.split(\"-\")[0])).astype(np.int16)\n",
    "        features.append(\"fecha_alta_year\")\n",
    "\n",
    "        # change date column to Y*12 + M float column\n",
    "        train_df[\"fecha_dato_float\"] = train_df[\"fecha_dato\"].map(date_to_float)\n",
    "        train_df[\"fecha_alta_float\"] = train_df[\"fecha_alta\"].map(date_to_float)\n",
    "\n",
    "        train_df[\"dato_minus_alta\"] = train_df[\"fecha_dato_float\"] - train_df[\"fecha_alta_float\"]\n",
    "        features.append(\"dato_minus_alta\")\n",
    "\n",
    "        train_df[\"int_date\"] = train_df[\"fecha_dato\"].map(date_to_int).astype(np.int8)\n",
    "\n",
    "        custom_one_hot(train_df, features, \"indresi\", {\"n\":\"N\"})\n",
    "        custom_one_hot(train_df, features, \"indext\", {\"s\":\"S\"})\n",
    "        custom_one_hot(train_df, features, \"conyuemp\", {\"n\":\"N\"})\n",
    "        custom_one_hot(train_df, features, \"sexo\", {\"h\":\"H\", \"v\":\"V\"})\n",
    "        custom_one_hot(train_df, features, \"ind_empleado\", {\"a\":\"A\", \"b\":\"B\", \"f\":\"F\", \"n\":\"N\"})\n",
    "        custom_one_hot(train_df, features, \"ind_nuevo\", {\"new\":1})\n",
    "        custom_one_hot(train_df, features, \"segmento\", {\"top\":\"01 - TOP\", \"particulares\":\"02 - PARTICULARES\", \"universitario\":\"03 - UNIVERSITARIO\"})\n",
    "        custom_one_hot(train_df, features, \"indfall\", {\"s\":\"S\"})\n",
    "\n",
    "        train_df[\"ind_actividad_cliente\"] = train_df[\"ind_actividad_cliente\"].map(lambda x: 0.0 if math.isnan(x) else x+1.0).astype(np.int8)\n",
    "        features.append(\"ind_actividad_cliente\")\n",
    "        custom_one_hot(train_df, features, \"indrel\", {\"1\":1, \"99\":99})\n",
    "        train_df[\"indrel_1mes\"] = train_df[\"indrel_1mes\"].map(lambda x: 5.0 if x == \"P\" else x).astype(float).fillna(0.0).astype(np.int8)\n",
    "        assert_uniq(train_df[\"indrel_1mes\"], \"indrel_1mes\")\n",
    "        features.append(\"indrel_1mes\")\n",
    "        custom_one_hot(train_df, features, \"tiprel_1mes\", {\"a\":\"A\", \"i\":\"I\", \"p\":\"P\", \"r\":\"R\"}, check=True)\n",
    "\n",
    "    return train_df, tuple(features)\n",
    "\n",
    "\n",
    "# make lag feature by addint step to int_date. Lag of products only\n",
    "def make_prev_df(train_df, step):\n",
    "    with Timer(\"make prev%s DF\" % step):\n",
    "        prev_df = pd.DataFrame()\n",
    "        prev_df[\"ncodpers\"] = train_df[\"ncodpers\"]\n",
    "        # added step per int_date to shift the data, generating a lag-feature\n",
    "        prev_df[\"int_date\"] = train_df[\"int_date\"].map(lambda x: x+step).astype(np.int8)\n",
    "        prod_features = [\"%s_prev%s\" % (prod, step) for prod in products]\n",
    "        for prod, prev in zip(products, prod_features):\n",
    "            prev_df[prev] = train_df[prod]\n",
    "    return prev_df, tuple(prod_features)\n",
    "\n",
    "\n",
    "# load train data and apply transforms\n",
    "def load_data(fname=\"data/8th.clean.all.csv\"):\n",
    "    with Timer(\"load train csv\"):\n",
    "        train_df = pd.read_csv(fname, dtype=dtypes)\n",
    "\n",
    "    with Timer(\"fill products NA\"):\n",
    "        for prod in products:\n",
    "            train_df[prod] = train_df[prod].fillna(0.0).astype(np.int8)\n",
    "\n",
    "    train_df, features = apply_transforms(train_df)\n",
    "\n",
    "    prev_dfs = []\n",
    "\n",
    "    prod_features = None\n",
    "\n",
    "    use_features = frozenset([1,2])\n",
    "    for step in range(1,6):\n",
    "        prev1_train_df, prod1_features = make_prev_df(train_df, step)\n",
    "        prev_dfs.append(prev1_train_df)\n",
    "        # use lag of 1,2 features\n",
    "        if step in use_features:\n",
    "            features += prod1_features\n",
    "        if step == 1:\n",
    "            prod_features = prod1_features\n",
    "\n",
    "    return train_df, prev_dfs, features, prod_features\n",
    "    # train_df = transformation applied to all data\n",
    "    # prev_dfs = 5 lag dataframes\n",
    "    # features = features to be used during transform + lag-1,2 features\n",
    "    # prod_features = lag-1 feature names\n",
    "\n",
    "# join existing dataframe with lag-5 produts columns\n",
    "def join_with_prev(df, prev_df, how):\n",
    "    with Timer(\"join %s\" % how):\n",
    "        assert set(df.columns.values.tolist()) & set(prev_df.columns.values.tolist()) == set([\"ncodpers\", \"int_date\"])\n",
    "        print(\"before join\", len(df))\n",
    "        df = df.merge(prev_df, on=[\"ncodpers\", \"int_date\"], how=how) ### LEARNT merging via two column conditions\n",
    "        for f in set(prev_df.columns.values.tolist()) - set([\"ncodpers\", \"int_date\"]):\n",
    "            df[f] = df[f].astype(np.float16)\n",
    "        print(\"after join\", len(df))\n",
    "        return df\n",
    "\n",
    "\n",
    "def make_data():\n",
    "    train_df, prev_dfs, features, prod_features = load_data()\n",
    "\n",
    "    for i, prev_df in enumerate(prev_dfs):\n",
    "        with Timer(\"join train with prev%s\" % (i+1)):\n",
    "            how = \"inner\" if i == 0 else \"left\" # WHY inner first and left?\n",
    "            train_df = join_with_prev(train_df, prev_df, how=how)\n",
    "\n",
    "    # Various aggregates to try\n",
    "    # for prod in products:\n",
    "    #     print()\n",
    "    #     print(prod)\n",
    "    #     #prev1_bin = (train_df[prod + \"_prev1\"] != 1).astype(np.int8)\n",
    "    #     for begin, end in [(2,5),(1,4)]:\n",
    "    #         prods = [\"%s_prev%s\" % (prod, i) for i in range(begin,end+1)]\n",
    "    #         mp_df = train_df.as_matrix(columns=prods)\n",
    "    #         print(prods)\n",
    "    #\n",
    "    #         stdf = \"%s_std_%s_%s\" % (prod,begin,end)\n",
    "    #         train_df[stdf] = np.nanstd(mp_df, axis=1) #  * prev1_bin\n",
    "    #\n",
    "    #         maxf = \"%s_max_%s_%s\"%(prod,begin,end)\n",
    "    #         train_df[maxf] = np.nanmax(mp_df, axis=1).astype(np.int8)\n",
    "    #\n",
    "    #         # minf = \"%s_min_%s_%s\"%(prod,begin,end)\n",
    "    #         # train_df[minf] = np.nanmin(mp_df, axis=1).astype(np.int8)\n",
    "    #\n",
    "    #         chf = \"%s_ch_%s_%s\"%(prod,begin,end)\n",
    "    #         train_df[chf] = np.sum(np.invert(np.isclose(mp_df[:,1:], mp_df[:,:-1], equal_nan=True)), axis=1, dtype=np.int8)\n",
    "    #\n",
    "    #         sumf = \"%s_sum_%s_%s\"%(prod,begin,end)\n",
    "    #         train_df[sumf] = np.nansum(mp_df, axis=1, dtype=np.int8)\n",
    "    #\n",
    "    #         skewf = \"%s_skew_%s_%s\"%(prod,begin,end)\n",
    "    #         train_df[skewf] = scipy.stats.skew(mp_df, axis=1)\n",
    "    #\n",
    "    #         features += (stdf,maxf,chf,sumf,skewf)\n",
    "\n",
    "    for prod in products:\n",
    "        print()\n",
    "        print(prod)\n",
    "        for begin, end in [(1,3),(1,5),(2,5)]: ### LEARNT iterate over list of tuples\n",
    "            prods = [\"%s_prev%s\" % (prod, i) for i in range(begin,end+1)]\n",
    "            mp_df = train_df.as_matrix(columns=prods) ### LEARNT subsetting dataframe to numpy array via as_matrix\n",
    "            print(prods)\n",
    "\n",
    "            stdf = \"%s_std_%s_%s\" % (prod,begin,end)\n",
    "            train_df[stdf] = np.nanstd(mp_df, axis=1) #  * prev1_bin\n",
    "\n",
    "            features += (stdf,)\n",
    "\n",
    "    for prod in products:\n",
    "        print()\n",
    "        print(prod)\n",
    "        for begin, end in [(2,3),(2,5)]:\n",
    "            prods = [\"%s_prev%s\" % (prod, i) for i in range(begin,end+1)]\n",
    "            mp_df = train_df.as_matrix(columns=prods)\n",
    "            print(prods)\n",
    "\n",
    "            minf = \"%s_min_%s_%s\"%(prod,begin,end)\n",
    "            train_df[minf] = np.nanmin(mp_df, axis=1).astype(np.int8)\n",
    "\n",
    "            maxf = \"%s_max_%s_%s\"%(prod,begin,end)\n",
    "            train_df[maxf] = np.nanmax(mp_df, axis=1).astype(np.int8)\n",
    "\n",
    "            features += (minf,maxf,)\n",
    "\n",
    "    with Timer(\"Remove unused columns\"):\n",
    "        leave_columns = [\"ncodpers\", \"int_date\", \"fecha_dato\"] + list(products) + list(features)\n",
    "        assert len(leave_columns) == len(set(leave_columns))\n",
    "        train_df = train_df[leave_columns]\n",
    "\n",
    "    return train_df, features, prod_features\n",
    "    # train_df = col-subsetted dataframe of trn/tst\n",
    "    # features = features to be used during transform + lag-1,2 features + additionals\n",
    "    # prod_features = lag-1 feature names\n",
    "\n",
    "all_df, features, prod_features = make_data()\n",
    "with Timer(\"save data\"):\n",
    "    all_df.to_pickle(\"data/8th.feature_engineer.all.pkl\")\n",
    "    pickle.dump((features, prod_features), open(\"data/8th.feature_engineer.cv_meta.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
