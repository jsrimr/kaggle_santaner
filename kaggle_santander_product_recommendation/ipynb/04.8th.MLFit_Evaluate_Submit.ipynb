{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import engines\n",
    "from utils import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gzip\n",
    "import io\n",
    "np.random.seed(2016)\n",
    "\n",
    "def make_submission(f, Y_test, C):\n",
    "    Y_ret = []\n",
    "    with Timer(\"make submission\"):\n",
    "        f.write(\"ncodpers,added_products\\n\".encode('utf-8'))\n",
    "        for c, y_test in zip(C, Y_test):\n",
    "            y_prods = [(y,p,ip) for y,p,ip in zip(y_test, products, range(len(products)))]\n",
    "            y_prods = sorted(y_prods, key=lambda a: a[0], reverse=True)[:7]\n",
    "            Y_ret.append([ip for y,p,ip in y_prods])\n",
    "            y_prods = [p for y,p,ip in y_prods]\n",
    "            f.write((\"%s,%s\\n\" % (int(c), \" \".join(y_prods))).encode('utf-8'))\n",
    "    return Y_ret\n",
    "\n",
    "\n",
    "# uses designates tr_date as test and all data before as train\n",
    "def train_predict(all_df, features, prod_features, str_date, cv):\n",
    "    test_date = date_to_int(str_date)\n",
    "    train_df = all_df[all_df.int_date < test_date]\n",
    "    test_df = pd.DataFrame(all_df[all_df.int_date == test_date])\n",
    "    print(sorted(set(train_df.columns.values.tolist()))) # print colnames\n",
    "    print(len(train_df.columns.values.tolist()), len(set(train_df.columns.values.tolist()))) # check duplicate\n",
    "    print(len(features),len(set(features))) # check duplicate\n",
    "\n",
    "    ### LEARNT clever, smart method to get the purchase\n",
    "    # subset train data to purchases only\n",
    "    # get single multi-class target as well\n",
    "    X = []\n",
    "    Y = []\n",
    "    for i, prod in enumerate(products):\n",
    "        prev = prod + \"_prev1\"\n",
    "        prX = train_df[(train_df[prod] == 1) & (train_df[prev] == 0)] # select those who purchased a product\n",
    "        prY = np.zeros(prX.shape[0], dtype=np.int8) + i\n",
    "        X.append(prX)\n",
    "        Y.append(prY)\n",
    "        print(prod, prX.shape)\n",
    "\n",
    "\n",
    "    XY = pd.concat(X)\n",
    "    Y = np.hstack(Y)\n",
    "    XY[\"y\"] = Y\n",
    "    XY[\"url\"] = np.zeros(len(XY), dtype=np.int8) ### WHY is this url here for?\n",
    "    # XY is now train data with actual purchases and its target labels\n",
    "\n",
    "    del train_df\n",
    "    del all_df\n",
    "\n",
    "\n",
    "    XY[\"ncodepers_fecha_dato\"] = XY[\"ncodpers\"].astype(str) + XY[\"fecha_dato\"]\n",
    "    uniqs, counts = np.unique(XY[\"ncodepers_fecha_dato\"], return_counts=True)\n",
    "    weights = np.exp(1/counts - 1)\n",
    "    # LEARNT giving exponential less weight to same user data in each month, due to multiple purchases\n",
    "    print(np.unique(counts, return_counts=True))\n",
    "    print(np.unique(weights, return_counts=True))\n",
    "    wdf = pd.DataFrame()\n",
    "    wdf[\"ncodepers_fecha_dato\"] = uniqs\n",
    "    wdf[\"counts\"] = counts\n",
    "    wdf[\"weight\"] = weights\n",
    "    print(\"before merge\", len(XY))\n",
    "    # merge unique counts and its weights to main data\n",
    "    XY = XY.merge(wdf, on=\"ncodepers_fecha_dato\")\n",
    "    print(\"after merge\", len(XY))\n",
    "\n",
    "    print(XY.shape)\n",
    "\n",
    "    mask = np.random.rand(len(XY)) < 0.8 # 80 percent as train, 20 percent as valid\n",
    "    XY_train = XY[mask]\n",
    "    XY_validate = XY[~mask]\n",
    "\n",
    "    with Timer(\"prepare test data\"):\n",
    "        test_df[\"y\"] = test_df[\"ncodpers\"]\n",
    "        test_df[\"url\"] = np.zeros(len(test_df), dtype=np.int8)\n",
    "        test_df[\"weight\"] = np.ones(len(test_df), dtype=np.int8) # weight of one to test data\n",
    "        Y_prev = test_df.as_matrix(columns=prod_features) # lag-1 products\n",
    "        C = test_df.as_matrix(columns=[\"ncodpers\"])\n",
    "        for prod in products:\n",
    "            prev = prod + \"_prev1\"\n",
    "            padd = prod + \"_add\"\n",
    "            test_df[padd] = test_df[prod] - test_df[prev]\n",
    "            # cv = True,  test_df has this value, and calculates purchase\n",
    "            # cv = False, test_df does not have test_df[prod] WHY\n",
    "        test_add_mat = test_df.as_matrix(columns=[prod + \"_add\" for prod in products])\n",
    "        test_add_list = [list() for i in range(len(C))] # list of empty list with size equal to test_df row\n",
    "        assert test_add_mat.shape == (len(C), len(products))\n",
    "        count = 0\n",
    "        for c in range(len(C)):\n",
    "            for p in range(len(products)):\n",
    "                if test_add_mat[c,p] > 0:\n",
    "                    test_add_list[c].append(p)\n",
    "                    count += 1\n",
    "        # test_add_list is a list of purchased items per user in test_df\n",
    "        # this is only useful for cv purpose, not for actual predcition on real test_df\n",
    "\n",
    "    if cv:\n",
    "        max_map7 = mapk(test_add_list, test_add_list, 7, 0.0)\n",
    "        map7coef = float(len(test_add_list)) / float(sum([int(bool(a)) for a in test_add_list]))\n",
    "        print(\"Max MAP@7\", str_date, max_map7, max_map7*map7coef)\n",
    "\n",
    "    with Timer(\"XGBoost\"):\n",
    "        Y_test_xgb = engines.xgboost(XY_train, XY_validate, test_df, features, XY_all = XY,\n",
    "            restore = (str_date == \"2016-06-28\")\n",
    "        )\n",
    "        # LEARNT doing Y_test_xgb - Y_prev is removing predictions if bought in prev month!\n",
    "        test_add_list_xgboost = make_submission(io.BytesIO() if cv else gzip.open(\"output/8th.%s.xgb.csv.gz\" % str_date, \"wb\"),\n",
    "                                                Y_test_xgb - Y_prev, C)\n",
    "        if cv:\n",
    "            map7xgboost = mapk(test_add_list, test_add_list_xgboost, 7, 0.0)\n",
    "            print(\"XGBoost MAP@7\", str_date, map7xgboost, map7xgboost*map7coef)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "all_df = pickle.load(open('data/8th.feature_engineer.all.pkl', 'rb'))\n",
    "features, prod_features = pickle.load(open('data/8th.feature_engineer.cv_meta.pkl', 'rb'))\n",
    "\n",
    "train_predict(all_df, features, prod_features, \"2016-05-28\", cv=True)\n",
    "train_predict(all_df, features, prod_features, \"2016-06-28\", cv=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "소요 시간 : ~3시간\n",
    "점수 : \n",
    "  - Public : 0.0305956\n",
    "  - Private : 0.0309524\n",
    "  - Rank : 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apk(actual, predicted, k=10, default=1.0):\n",
    "    if len(predicted)>k:\n",
    "        predicted = predicted[:k]\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i,p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i+1.0)\n",
    "\n",
    "    if not actual:\n",
    "        return default\n",
    "\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "def mapk(actual, predicted, k=10, default=1.0):\n",
    "    return np.mean([apk(a,p,k,default) for a,p in zip(actual, predicted)]) ### LEARNT good use of zip in loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_lgbm = engines.lightgbm(XY_train, XY_validate, test_df, features, XY_all = XY, restore = (str_date == \"2016-06-28\"))\n",
    "test_add_list_lightgbm = make_submission(io.BytesIO() if cv else gzip.open(\"tmp/%s.lightgbm.csv.gz\" % str_date, \"wb\"), Y_test_lgbm - Y_prev, C)\n",
    "\n",
    "if cv:\n",
    "    map7lightgbm = mapk(test_add_list, test_add_list_lightgbm, 7, 0.0)\n",
    "    print(\"LightGBMlib MAP@7\", str_date, map7lightgbm, map7lightgbm * map7coef)\n",
    "\n",
    "Y_test_xgb = engines.xgboost(XY_train, XY_validate, test_df, features, XY_all = XY, restore = (str_date == \"2016-06-28\"))\n",
    "test_add_list_xgboost = make_submission(io.BytesIO() if cv else gzip.open(\"tmp/%s.xgboost.csv.gz\" % str_date, \"wb\"), Y_test_xgb - Y_prev, C)\n",
    "\n",
    "if cv:\n",
    "    map7xgboost = mapk(test_add_list, test_add_list_xgboost, 7, 0.0)\n",
    "    print(\"XGBoost MAP@7\", str_date, map7xgboost, map7xgboost * map7coef)\n",
    "\n",
    "Y_test = np.sqrt(np.multiply(Y_test_xgb, Y_test_lgbm))\n",
    "test_add_list_xl = make_submission(io.BytesIO() if cv else gzip.open(\"tmp/%s.xgboost-lightgbm.csv.gz\" % str_date, \"wb\"), Y_test - Y_prev, C)\n",
    "if cv:\n",
    "    map7xl = mapk(test_add_list, test_add_list_xl, 7, 0.0)\n",
    "    print(\"XGBoost + LightGBM MAP@7\", str_date, map7xl, map7xl * map7coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import xgboost as xgb\n",
    "#import lightgbm as lgbm\n",
    "\n",
    "from utils import *\n",
    "\n",
    "\n",
    "def xgboost(XY_train, XY_validate, test_df, features, XY_all=None, restore=False):\n",
    "    param = {\n",
    "        'objective': 'multi:softprob',\n",
    "        'eta': 0.1,\n",
    "        'min_child_weight': 10,\n",
    "        'max_depth': 8,\n",
    "        'silent': 1,\n",
    "        # 'nthread': 16,\n",
    "        'eval_metric': 'mlogloss',\n",
    "        'colsample_bytree': 0.8,\n",
    "        'colsample_bylevel': 0.9,\n",
    "        'num_class': len(products),\n",
    "    }\n",
    "\n",
    "    if not restore:\n",
    "        ### LEARNT smart way to store all in pandas and split in numpy at last\n",
    "        X_train = XY_train.as_matrix(columns=features)\n",
    "        Y_train = XY_train.as_matrix(columns=[\"y\"])\n",
    "        W_train = XY_train.as_matrix(columns=[\"weight\"]) ### LEARNT use of weights in xgboost\n",
    "        train = xgb.DMatrix(X_train, label=Y_train, feature_names=features, weight=W_train)\n",
    "\n",
    "        X_validate = XY_validate.as_matrix(columns=features)\n",
    "        Y_validate = XY_validate.as_matrix(columns=[\"y\"])\n",
    "        W_validate = XY_validate.as_matrix(columns=[\"weight\"])\n",
    "        validate = xgb.DMatrix(X_validate, label=Y_validate, feature_names=features, weight=W_validate)\n",
    "\n",
    "        print(param)\n",
    "        evallist  = [(train,'train'), (validate,'eval')]\n",
    "        model = xgb.train(param, train, 1000, evals=evallist, early_stopping_rounds=20)\n",
    "        pickle.dump(model, open(\"next_multi.pickle\", \"wb\")) # dump xgboost model\n",
    "\n",
    "    else:\n",
    "        model = pickle.load(open(\"next_multi.pickle\", \"rb\"))\n",
    "    best_ntree_limit = model.best_ntree_limit\n",
    "\n",
    "    if XY_all is not None:\n",
    "        X_all = XY_all.as_matrix(columns=features)\n",
    "        Y_all = XY_all.as_matrix(columns=[\"y\"])\n",
    "        W_all = XY_all.as_matrix(columns=[\"weight\"])\n",
    "        all_data = xgb.DMatrix(X_all, label=Y_all, feature_names=features, weight=W_all)\n",
    "\n",
    "        evallist  = [(all_data,'all_data')]\n",
    "        # balance num_round (==num tree)\n",
    "        best_ntree_limit = int(best_ntree_limit * (len(XY_train) + len(XY_validate)) / len(XY_train))\n",
    "        model = xgb.train(param, all_data, best_ntree_limit, evals=evallist)\n",
    "\n",
    "    print(\"Feature importance:\")\n",
    "    for kv in sorted([(k,v) for k,v in model.get_fscore().items()], key=lambda kv: kv[1], reverse=True):\n",
    "        print(kv)\n",
    "\n",
    "    X_test = test_df.as_matrix(columns=features)\n",
    "    test = xgb.DMatrix(X_test, feature_names=features)\n",
    "\n",
    "    return model.predict(test, ntree_limit=best_ntree_limit)\n",
    "\n",
    "\n",
    "def lightgbm(XY_train, XY_validate, test_df, features, XY_all=None, restore=False):\n",
    "    train = lgbm.Dataset(XY_train[list(features)], label=XY_train[\"y\"], weight=XY_train[\"weight\"], feature_name=features)\n",
    "    validate = lgbm.Dataset(XY_validate[list(features)], label=XY_validate[\"y\"], weight=XY_validate[\"weight\"], feature_name=features, reference=train)\n",
    "\n",
    "    params = {\n",
    "        'task' : 'train',\n",
    "        'boosting_type' : 'gbdt',\n",
    "        'objective' : 'multiclass',\n",
    "        'num_class': 24,\n",
    "        'metric' : {'multi_logloss'},\n",
    "        'is_training_metric': True,\n",
    "        'max_bin': 255,\n",
    "        'num_leaves' : 64,\n",
    "        'learning_rate' : 0.1,\n",
    "        'feature_fraction' : 0.8,\n",
    "        'min_data_in_leaf': 10,\n",
    "        'min_sum_hessian_in_leaf': 5,\n",
    "        # 'num_threads': 16,\n",
    "    }\n",
    "    print(params)\n",
    "\n",
    "    if not restore:\n",
    "        model = lgbm.train(params, train, num_boost_round=1000, valid_sets=validate, early_stopping_rounds=20)\n",
    "        best_iteration = model.best_iteration\n",
    "        model.save_model(\"tmp/lgbm.model.txt\")\n",
    "        pickle.dump(best_iteration, open(\"tmp/lgbm.model.meta\", \"wb\"))\n",
    "    else:\n",
    "        model = lgbm.Booster(model_file=\"tmp/lgbm.model.txt\")\n",
    "        best_iteration = pickle.load(open(\"tmp/lgbm.model.meta\", \"rb\"))\n",
    "\n",
    "    if XY_all is not None:\n",
    "        best_iteration = int(best_iteration * len(XY_all) / len(XY_train))\n",
    "        all_train = lgbm.Dataset(XY_all[list(features)], label=XY_all[\"y\"], weight=XY_all[\"weight\"], feature_name=features)\n",
    "        model = lgbm.train(params, all_train, num_boost_round=best_iteration)\n",
    "        model.save_model(\"tmp/lgbm.all.model.txt\")\n",
    "\n",
    "    print(\"Feature importance by split:\")\n",
    "    for kv in sorted([(k,v) for k,v in zip(features, model.feature_importance(\"split\"))], key=lambda kv: kv[1], reverse=True):\n",
    "        print(kv)\n",
    "    print(\"Feature importance by gain:\")\n",
    "    for kv in sorted([(k,v) for k,v in zip(features, model.feature_importance(\"gain\"))], key=lambda kv: kv[1], reverse=True):\n",
    "        print(kv)\n",
    "\n",
    "    return model.predict(test_df[list(features)], num_iteration=best_iteration)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
