{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "np.random.seed(2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apk(actual, predicted, k=7, default=0.0):\n",
    "    if len(predicted) > k:\n",
    "        predicted = predicted[:k]\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "    for i, p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i+1.0)\n",
    "\n",
    "    if not actual:\n",
    "        return default\n",
    "\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "def mapk(actual, predicted, k=7, default=0.0):\n",
    "    return np.mean([apk(a, p, k, default) for a, p in zip(actual, predicted)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 불러온다.\n",
    "trn = pd.read_csv('data/train_ver2.csv')\n",
    "tst = pd.read_csv('data/test_ver2.csv')\n",
    "\n",
    "## 데이터 전처리 ##\n",
    "\n",
    "# 제품 변수를 별도로 저장해 놓는다.\n",
    "prods = trn.columns[24:].tolist()\n",
    "\n",
    "# 제품 변수 결측값을 미리 0으로 대체한다.\n",
    "trn[prods] = trn[prods].fillna(0.0).astype(np.int8)\n",
    "\n",
    "# 24개 제품 중 하나도 보유하지 않는 고객 데이터를 제거한다.\n",
    "no_product = trn[prods].sum(axis=1) == 0\n",
    "trn = trn[~no_product]\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터를 통합한다. 테스트 데이터에 없는 제품 변수는 0으로 채운다.\n",
    "for col in trn.columns[24:]:\n",
    "    tst[col] = 0\n",
    "df = pd.concat([trn, tst], axis=0)\n",
    "\n",
    "# 학습에 사용할 변수를 담는 list이다.\n",
    "features = []\n",
    "\n",
    "# 범주형 변수를 .factorize() 함수를 통해 label encoding한다.\n",
    "categorical_cols = ['ind_empleado', 'pais_residencia', 'sexo', 'tiprel_1mes', 'indresi', 'indext', 'conyuemp', \n",
    "                'canal_entrada', 'indfall', 'tipodom', 'nomprov', 'segmento']\n",
    "for col in categorical_cols:\n",
    "    df[col], _ = df[col].factorize(na_sentinel=-99)\n",
    "features += categorical_cols\n",
    "\n",
    "# 수치형 변수의 특이값과 결측값을 -99로 대체하고, 정수형으로 변환한다.\n",
    "df['age'].replace(' NA', -99, inplace=True)\n",
    "df['age'] = df['age'].astype(np.int8)\n",
    "\n",
    "df['antiguedad'].replace('     NA', -99, inplace=True)\n",
    "df['antiguedad'] = df['antiguedad'].astype(np.int8)\n",
    "\n",
    "df['renta'].replace('         NA', -99, inplace=True)\n",
    "df['renta'].fillna(-99, inplace=True)\n",
    "df['renta'] = df['renta'].astype(float).astype(np.int8)\n",
    "\n",
    "df['indrel_1mes'].replace('P', 5, inplace=True)\n",
    "df['indrel_1mes'].fillna(-99, inplace=True)\n",
    "df['indrel_1mes'] = df['indrel_1mes'].astype(float).astype(np.int8)\n",
    "\n",
    "features += ['age','antiguedad','renta','ind_nuevo','indrel','indrel_1mes','ind_actividad_cliente']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 날짜 관련 변수에 간단한 피쳐 엔지니어링을 수행한다.\n",
    "# 두 날짜 변수에서 연도와 월 데이터를 추출한다.\n",
    "df['fecha_alta_month'] = df['fecha_alta'].map(lambda x: 0.0 if x.__class__ is float else float(x.split('-')[1])).astype(np.int8)\n",
    "df['fecha_alta_year'] = df['fecha_alta'].map(lambda x: 0.0 if x.__class__ is float else float(x.split('-')[0])).astype(np.int16)\n",
    "features += ['fecha_alta_month', 'fecha_alta_year']\n",
    "\n",
    "df['ult_fec_cli_1t_month'] = df['ult_fec_cli_1t'].map(lambda x: 0.0 if x.__class__ is float else float(x.split('-')[1])).astype(np.int8)\n",
    "df['ult_fec_cli_1t_year'] = df['ult_fec_cli_1t'].map(lambda x: 0.0 if x.__class__ is float else float(x.split('-')[0])).astype(np.int16)\n",
    "features += ['ult_fec_cli_1t_month', 'ult_fec_cli_1t_year']\n",
    "\n",
    "# 그 외 변수의 결측값은 모두 -99로 대체한다.\n",
    "df.fillna(-99, inplace=True)\n",
    "\n",
    "# (피쳐 엔지니어링) lag 데이터를 생성한다.\n",
    "# 코드 1-12와 유사한 코드 흐름이다.\n",
    "\n",
    "# 날짜를 숫자로 변환하는 함수이다. 2015-01-28은 1, 2016-06-28은 18로 변환된다\n",
    "def date_to_int(str_date):\n",
    "    Y, M, D = [int(a) for a in str_date.strip().split(\"-\")] \n",
    "    int_date = (int(Y) - 2015) * 12 + int(M)\n",
    "    return int_date\n",
    "\n",
    "# 날짜를 숫자로 변환하여 int_date에 저장한다\n",
    "df['int_date'] = df['fecha_dato'].map(date_to_int).astype(np.int8)\n",
    "\n",
    "# 데이터를 복사하고, int_date 날짜에 1을 더하여 lag를 생성한다. 변수명에 _prev를 추가한다.\n",
    "df_lag = df.copy()\n",
    "df_lag.columns = [col + '_prev' if col not in ['ncodpers', 'int_date'] else col for col in df.columns ]\n",
    "df_lag['int_date'] += 1\n",
    "\n",
    "# 원본 데이터와 lag 데이터를 ncodper와 int_date 기준으로 합친다. Lag 데이터의 int_date는 1 밀려있기 때문에, 저번달의 제품 정보가 삽입된다.\n",
    "df_trn = df.merge(df_lag, on=['ncodpers','int_date'], how='left')\n",
    "\n",
    "# 메모리 효율을 위해 불필요한 변수를 메모리에서 제거한다\n",
    "del df, df_lag\n",
    "\n",
    "# 저번달의 제품 정보가 존재하지 않을 경우를 대비하여 0으로 대체한다.\n",
    "for prod in prods:\n",
    "    prev = prod + '_prev'\n",
    "    df_trn[prev].fillna(0, inplace=True)\n",
    "df_trn.fillna(-99, inplace=True)\n",
    "\n",
    "# lag 변수를 추가한다.\n",
    "features += [feature + '_prev' for feature in features]\n",
    "features += [prod + '_prev' for prod in prods]\n",
    "\n",
    "###\n",
    "### Baseline 모델 이후에, 다양한 피쳐 엔지니어링을 여기에 추가한다.\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/irteam/.pyenv/versions/3.6.4/envs/venv/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (5,8,11,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/home1/irteam/.pyenv/versions/3.6.4/envs/venv/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:2.73433\teval-mlogloss:2.74233\n",
      "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mlogloss hasn't improved in 20 rounds.\n",
      "[1]\ttrain-mlogloss:2.48344\teval-mlogloss:2.49555\n",
      "[2]\ttrain-mlogloss:2.30469\teval-mlogloss:2.31939\n",
      "[3]\ttrain-mlogloss:2.15884\teval-mlogloss:2.17532\n",
      "[4]\ttrain-mlogloss:2.03811\teval-mlogloss:2.05536\n",
      "[5]\ttrain-mlogloss:1.9436\teval-mlogloss:1.96203\n",
      "[6]\ttrain-mlogloss:1.86333\teval-mlogloss:1.88254\n",
      "[7]\ttrain-mlogloss:1.7903\teval-mlogloss:1.81012\n",
      "[8]\ttrain-mlogloss:1.73062\teval-mlogloss:1.75126\n",
      "[9]\ttrain-mlogloss:1.67667\teval-mlogloss:1.69764\n",
      "[10]\ttrain-mlogloss:1.62686\teval-mlogloss:1.64816\n",
      "[11]\ttrain-mlogloss:1.58347\teval-mlogloss:1.60491\n",
      "[12]\ttrain-mlogloss:1.54526\teval-mlogloss:1.56676\n",
      "[13]\ttrain-mlogloss:1.5097\teval-mlogloss:1.53164\n",
      "[14]\ttrain-mlogloss:1.47852\teval-mlogloss:1.50091\n",
      "[15]\ttrain-mlogloss:1.4491\teval-mlogloss:1.47187\n",
      "[16]\ttrain-mlogloss:1.42437\teval-mlogloss:1.44772\n",
      "[17]\ttrain-mlogloss:1.40017\teval-mlogloss:1.42392\n",
      "[18]\ttrain-mlogloss:1.37815\teval-mlogloss:1.40214\n",
      "[19]\ttrain-mlogloss:1.35801\teval-mlogloss:1.38243\n",
      "[20]\ttrain-mlogloss:1.33967\teval-mlogloss:1.36419\n",
      "[21]\ttrain-mlogloss:1.32261\teval-mlogloss:1.34743\n",
      "[22]\ttrain-mlogloss:1.30671\teval-mlogloss:1.33198\n",
      "[23]\ttrain-mlogloss:1.29188\teval-mlogloss:1.31768\n",
      "[24]\ttrain-mlogloss:1.27771\teval-mlogloss:1.30382\n",
      "[25]\ttrain-mlogloss:1.26402\teval-mlogloss:1.29045\n",
      "[26]\ttrain-mlogloss:1.2515\teval-mlogloss:1.27821\n",
      "[27]\ttrain-mlogloss:1.24025\teval-mlogloss:1.26718\n",
      "[28]\ttrain-mlogloss:1.2292\teval-mlogloss:1.25629\n",
      "[29]\ttrain-mlogloss:1.21916\teval-mlogloss:1.24664\n",
      "[30]\ttrain-mlogloss:1.2097\teval-mlogloss:1.23765\n",
      "[31]\ttrain-mlogloss:1.20117\teval-mlogloss:1.22948\n",
      "[32]\ttrain-mlogloss:1.19273\teval-mlogloss:1.22129\n",
      "[33]\ttrain-mlogloss:1.18499\teval-mlogloss:1.21387\n",
      "[34]\ttrain-mlogloss:1.17783\teval-mlogloss:1.20696\n",
      "[35]\ttrain-mlogloss:1.17131\teval-mlogloss:1.20078\n",
      "[36]\ttrain-mlogloss:1.16475\teval-mlogloss:1.19476\n",
      "[37]\ttrain-mlogloss:1.15886\teval-mlogloss:1.18919\n",
      "[38]\ttrain-mlogloss:1.15308\teval-mlogloss:1.18381\n",
      "[39]\ttrain-mlogloss:1.14785\teval-mlogloss:1.1789\n",
      "[40]\ttrain-mlogloss:1.14292\teval-mlogloss:1.17453\n",
      "[41]\ttrain-mlogloss:1.13802\teval-mlogloss:1.16998\n",
      "[42]\ttrain-mlogloss:1.13344\teval-mlogloss:1.1658\n",
      "[43]\ttrain-mlogloss:1.12901\teval-mlogloss:1.1618\n",
      "[44]\ttrain-mlogloss:1.12466\teval-mlogloss:1.15794\n",
      "[45]\ttrain-mlogloss:1.12089\teval-mlogloss:1.15457\n",
      "[46]\ttrain-mlogloss:1.11716\teval-mlogloss:1.15122\n",
      "[47]\ttrain-mlogloss:1.11402\teval-mlogloss:1.14851\n",
      "[48]\ttrain-mlogloss:1.11082\teval-mlogloss:1.14568\n",
      "[49]\ttrain-mlogloss:1.10773\teval-mlogloss:1.14291\n",
      "[50]\ttrain-mlogloss:1.10483\teval-mlogloss:1.14041\n",
      "[51]\ttrain-mlogloss:1.10181\teval-mlogloss:1.1378\n",
      "[52]\ttrain-mlogloss:1.099\teval-mlogloss:1.13533\n",
      "[53]\ttrain-mlogloss:1.09625\teval-mlogloss:1.1331\n",
      "[54]\ttrain-mlogloss:1.09371\teval-mlogloss:1.13097\n",
      "[55]\ttrain-mlogloss:1.09137\teval-mlogloss:1.12901\n",
      "[56]\ttrain-mlogloss:1.08917\teval-mlogloss:1.12708\n",
      "[57]\ttrain-mlogloss:1.08705\teval-mlogloss:1.12531\n",
      "[58]\ttrain-mlogloss:1.08484\teval-mlogloss:1.12356\n",
      "[59]\ttrain-mlogloss:1.08281\teval-mlogloss:1.12192\n",
      "[60]\ttrain-mlogloss:1.0809\teval-mlogloss:1.12033\n",
      "[61]\ttrain-mlogloss:1.0791\teval-mlogloss:1.11894\n",
      "[62]\ttrain-mlogloss:1.07725\teval-mlogloss:1.11757\n",
      "[63]\ttrain-mlogloss:1.0756\teval-mlogloss:1.11623\n",
      "[64]\ttrain-mlogloss:1.0739\teval-mlogloss:1.11498\n",
      "[65]\ttrain-mlogloss:1.07237\teval-mlogloss:1.11374\n",
      "[66]\ttrain-mlogloss:1.07085\teval-mlogloss:1.11266\n",
      "[67]\ttrain-mlogloss:1.06939\teval-mlogloss:1.11158\n",
      "[68]\ttrain-mlogloss:1.06802\teval-mlogloss:1.11048\n",
      "[69]\ttrain-mlogloss:1.06654\teval-mlogloss:1.10945\n",
      "[70]\ttrain-mlogloss:1.06519\teval-mlogloss:1.10847\n",
      "[71]\ttrain-mlogloss:1.06393\teval-mlogloss:1.10772\n",
      "[72]\ttrain-mlogloss:1.06271\teval-mlogloss:1.10694\n",
      "[73]\ttrain-mlogloss:1.06139\teval-mlogloss:1.10612\n",
      "[74]\ttrain-mlogloss:1.06016\teval-mlogloss:1.10532\n",
      "[75]\ttrain-mlogloss:1.05894\teval-mlogloss:1.10459\n",
      "[76]\ttrain-mlogloss:1.05783\teval-mlogloss:1.10394\n",
      "[77]\ttrain-mlogloss:1.05668\teval-mlogloss:1.10325\n",
      "[78]\ttrain-mlogloss:1.05568\teval-mlogloss:1.10272\n",
      "[79]\ttrain-mlogloss:1.05463\teval-mlogloss:1.10211\n",
      "[80]\ttrain-mlogloss:1.05373\teval-mlogloss:1.10153\n",
      "[81]\ttrain-mlogloss:1.05282\teval-mlogloss:1.10103\n",
      "[82]\ttrain-mlogloss:1.05183\teval-mlogloss:1.10049\n",
      "[83]\ttrain-mlogloss:1.05086\teval-mlogloss:1.10001\n",
      "[84]\ttrain-mlogloss:1.04987\teval-mlogloss:1.09955\n",
      "[85]\ttrain-mlogloss:1.04894\teval-mlogloss:1.09902\n",
      "[86]\ttrain-mlogloss:1.04801\teval-mlogloss:1.09852\n",
      "[87]\ttrain-mlogloss:1.04727\teval-mlogloss:1.09804\n",
      "[88]\ttrain-mlogloss:1.04656\teval-mlogloss:1.09763\n",
      "[89]\ttrain-mlogloss:1.04579\teval-mlogloss:1.09723\n",
      "[90]\ttrain-mlogloss:1.04507\teval-mlogloss:1.09686\n",
      "[91]\ttrain-mlogloss:1.04422\teval-mlogloss:1.09653\n",
      "[92]\ttrain-mlogloss:1.04335\teval-mlogloss:1.09612\n",
      "[93]\ttrain-mlogloss:1.0426\teval-mlogloss:1.09577\n",
      "[94]\ttrain-mlogloss:1.04182\teval-mlogloss:1.09547\n",
      "[95]\ttrain-mlogloss:1.04114\teval-mlogloss:1.09519\n",
      "[96]\ttrain-mlogloss:1.04041\teval-mlogloss:1.0949\n",
      "[97]\ttrain-mlogloss:1.03972\teval-mlogloss:1.09465\n",
      "[98]\ttrain-mlogloss:1.03905\teval-mlogloss:1.09439\n",
      "[99]\ttrain-mlogloss:1.03828\teval-mlogloss:1.09419\n",
      "[100]\ttrain-mlogloss:1.03761\teval-mlogloss:1.09398\n",
      "[101]\ttrain-mlogloss:1.03688\teval-mlogloss:1.09377\n",
      "[102]\ttrain-mlogloss:1.03617\teval-mlogloss:1.09357\n",
      "[103]\ttrain-mlogloss:1.03549\teval-mlogloss:1.09329\n",
      "[104]\ttrain-mlogloss:1.03489\teval-mlogloss:1.09307\n",
      "[105]\ttrain-mlogloss:1.03418\teval-mlogloss:1.09287\n",
      "[106]\ttrain-mlogloss:1.03359\teval-mlogloss:1.09268\n",
      "[107]\ttrain-mlogloss:1.03296\teval-mlogloss:1.09251\n",
      "[108]\ttrain-mlogloss:1.03236\teval-mlogloss:1.09236\n",
      "[109]\ttrain-mlogloss:1.03176\teval-mlogloss:1.09217\n",
      "[110]\ttrain-mlogloss:1.03116\teval-mlogloss:1.092\n",
      "[111]\ttrain-mlogloss:1.03049\teval-mlogloss:1.09187\n",
      "[112]\ttrain-mlogloss:1.02967\teval-mlogloss:1.09168\n",
      "[113]\ttrain-mlogloss:1.02903\teval-mlogloss:1.0916\n",
      "[114]\ttrain-mlogloss:1.02852\teval-mlogloss:1.09145\n",
      "[115]\ttrain-mlogloss:1.02794\teval-mlogloss:1.0913\n",
      "[116]\ttrain-mlogloss:1.02737\teval-mlogloss:1.09118\n",
      "[117]\ttrain-mlogloss:1.02666\teval-mlogloss:1.09104\n",
      "[118]\ttrain-mlogloss:1.02604\teval-mlogloss:1.09091\n",
      "[119]\ttrain-mlogloss:1.02549\teval-mlogloss:1.09088\n",
      "[120]\ttrain-mlogloss:1.02494\teval-mlogloss:1.09073\n",
      "[121]\ttrain-mlogloss:1.02428\teval-mlogloss:1.09063\n",
      "[122]\ttrain-mlogloss:1.02381\teval-mlogloss:1.09056\n",
      "[123]\ttrain-mlogloss:1.02323\teval-mlogloss:1.09048\n",
      "[124]\ttrain-mlogloss:1.02262\teval-mlogloss:1.09031\n",
      "[125]\ttrain-mlogloss:1.02179\teval-mlogloss:1.09021\n",
      "[126]\ttrain-mlogloss:1.02106\teval-mlogloss:1.09008\n",
      "[127]\ttrain-mlogloss:1.02033\teval-mlogloss:1.08991\n",
      "[128]\ttrain-mlogloss:1.01962\teval-mlogloss:1.08975\n",
      "[129]\ttrain-mlogloss:1.01916\teval-mlogloss:1.08967\n",
      "[130]\ttrain-mlogloss:1.01856\teval-mlogloss:1.08961\n",
      "[131]\ttrain-mlogloss:1.01802\teval-mlogloss:1.08954\n",
      "[132]\ttrain-mlogloss:1.01742\teval-mlogloss:1.08947\n",
      "[133]\ttrain-mlogloss:1.01692\teval-mlogloss:1.08943\n",
      "[134]\ttrain-mlogloss:1.01619\teval-mlogloss:1.08933\n",
      "[135]\ttrain-mlogloss:1.01556\teval-mlogloss:1.08927\n",
      "[136]\ttrain-mlogloss:1.01493\teval-mlogloss:1.08925\n",
      "[137]\ttrain-mlogloss:1.0144\teval-mlogloss:1.08921\n",
      "[138]\ttrain-mlogloss:1.01376\teval-mlogloss:1.0892\n",
      "[139]\ttrain-mlogloss:1.01329\teval-mlogloss:1.08908\n",
      "[140]\ttrain-mlogloss:1.01257\teval-mlogloss:1.08901\n",
      "[141]\ttrain-mlogloss:1.01207\teval-mlogloss:1.08897\n",
      "[142]\ttrain-mlogloss:1.01153\teval-mlogloss:1.08894\n",
      "[143]\ttrain-mlogloss:1.01095\teval-mlogloss:1.08893\n",
      "[144]\ttrain-mlogloss:1.01047\teval-mlogloss:1.08889\n",
      "[145]\ttrain-mlogloss:1.00999\teval-mlogloss:1.08885\n",
      "[146]\ttrain-mlogloss:1.00939\teval-mlogloss:1.08878\n",
      "[147]\ttrain-mlogloss:1.00883\teval-mlogloss:1.08874\n",
      "[148]\ttrain-mlogloss:1.00825\teval-mlogloss:1.08873\n",
      "[149]\ttrain-mlogloss:1.0077\teval-mlogloss:1.08869\n",
      "[150]\ttrain-mlogloss:1.00701\teval-mlogloss:1.08859\n",
      "[151]\ttrain-mlogloss:1.00643\teval-mlogloss:1.08858\n",
      "[152]\ttrain-mlogloss:1.00591\teval-mlogloss:1.08858\n",
      "[153]\ttrain-mlogloss:1.00537\teval-mlogloss:1.08853\n",
      "[154]\ttrain-mlogloss:1.0049\teval-mlogloss:1.08854\n",
      "[155]\ttrain-mlogloss:1.00429\teval-mlogloss:1.08849\n",
      "[156]\ttrain-mlogloss:1.00357\teval-mlogloss:1.0884\n",
      "[157]\ttrain-mlogloss:1.00291\teval-mlogloss:1.08833\n",
      "[158]\ttrain-mlogloss:1.00237\teval-mlogloss:1.08828\n",
      "[159]\ttrain-mlogloss:1.00182\teval-mlogloss:1.08827\n",
      "[160]\ttrain-mlogloss:1.00128\teval-mlogloss:1.08821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[161]\ttrain-mlogloss:1.00075\teval-mlogloss:1.0882\n",
      "[162]\ttrain-mlogloss:1.00029\teval-mlogloss:1.08818\n",
      "[163]\ttrain-mlogloss:0.999656\teval-mlogloss:1.08817\n",
      "[164]\ttrain-mlogloss:0.99896\teval-mlogloss:1.08808\n",
      "[165]\ttrain-mlogloss:0.998502\teval-mlogloss:1.08805\n",
      "[166]\ttrain-mlogloss:0.997958\teval-mlogloss:1.088\n",
      "[167]\ttrain-mlogloss:0.997552\teval-mlogloss:1.08797\n",
      "[168]\ttrain-mlogloss:0.996991\teval-mlogloss:1.08792\n",
      "[169]\ttrain-mlogloss:0.996497\teval-mlogloss:1.08786\n",
      "[170]\ttrain-mlogloss:0.995874\teval-mlogloss:1.08778\n",
      "[171]\ttrain-mlogloss:0.995408\teval-mlogloss:1.08777\n",
      "[172]\ttrain-mlogloss:0.994973\teval-mlogloss:1.0878\n",
      "[173]\ttrain-mlogloss:0.994349\teval-mlogloss:1.08772\n",
      "[174]\ttrain-mlogloss:0.993737\teval-mlogloss:1.08768\n",
      "[175]\ttrain-mlogloss:0.993237\teval-mlogloss:1.08774\n",
      "[176]\ttrain-mlogloss:0.99276\teval-mlogloss:1.08772\n",
      "[177]\ttrain-mlogloss:0.992139\teval-mlogloss:1.08769\n",
      "[178]\ttrain-mlogloss:0.991657\teval-mlogloss:1.08769\n",
      "[179]\ttrain-mlogloss:0.991029\teval-mlogloss:1.08763\n",
      "[180]\ttrain-mlogloss:0.990391\teval-mlogloss:1.08764\n",
      "[181]\ttrain-mlogloss:0.989698\teval-mlogloss:1.08762\n",
      "[182]\ttrain-mlogloss:0.989198\teval-mlogloss:1.08761\n",
      "[183]\ttrain-mlogloss:0.988656\teval-mlogloss:1.08764\n",
      "[184]\ttrain-mlogloss:0.988042\teval-mlogloss:1.0876\n",
      "[185]\ttrain-mlogloss:0.987521\teval-mlogloss:1.0876\n",
      "[186]\ttrain-mlogloss:0.987059\teval-mlogloss:1.0876\n",
      "[187]\ttrain-mlogloss:0.986647\teval-mlogloss:1.08757\n",
      "[188]\ttrain-mlogloss:0.986234\teval-mlogloss:1.08758\n",
      "[189]\ttrain-mlogloss:0.985808\teval-mlogloss:1.08755\n",
      "[190]\ttrain-mlogloss:0.985215\teval-mlogloss:1.08758\n",
      "[191]\ttrain-mlogloss:0.984527\teval-mlogloss:1.08752\n",
      "[192]\ttrain-mlogloss:0.983989\teval-mlogloss:1.08753\n",
      "[193]\ttrain-mlogloss:0.983536\teval-mlogloss:1.0875\n",
      "[194]\ttrain-mlogloss:0.982902\teval-mlogloss:1.08749\n",
      "[195]\ttrain-mlogloss:0.98243\teval-mlogloss:1.0875\n",
      "[196]\ttrain-mlogloss:0.981876\teval-mlogloss:1.08746\n",
      "[197]\ttrain-mlogloss:0.981273\teval-mlogloss:1.08739\n",
      "[198]\ttrain-mlogloss:0.980824\teval-mlogloss:1.08737\n",
      "[199]\ttrain-mlogloss:0.980385\teval-mlogloss:1.0874\n",
      "[200]\ttrain-mlogloss:0.979828\teval-mlogloss:1.08737\n",
      "[201]\ttrain-mlogloss:0.979216\teval-mlogloss:1.08734\n",
      "[202]\ttrain-mlogloss:0.978647\teval-mlogloss:1.08728\n",
      "[203]\ttrain-mlogloss:0.978138\teval-mlogloss:1.08729\n",
      "[204]\ttrain-mlogloss:0.97772\teval-mlogloss:1.08729\n",
      "[205]\ttrain-mlogloss:0.977115\teval-mlogloss:1.08727\n",
      "[206]\ttrain-mlogloss:0.976547\teval-mlogloss:1.0872\n",
      "[207]\ttrain-mlogloss:0.975935\teval-mlogloss:1.0872\n",
      "[208]\ttrain-mlogloss:0.975308\teval-mlogloss:1.08717\n",
      "[209]\ttrain-mlogloss:0.974714\teval-mlogloss:1.0872\n",
      "[210]\ttrain-mlogloss:0.974213\teval-mlogloss:1.08716\n",
      "[211]\ttrain-mlogloss:0.973717\teval-mlogloss:1.08716\n",
      "[212]\ttrain-mlogloss:0.973238\teval-mlogloss:1.08714\n",
      "[213]\ttrain-mlogloss:0.97276\teval-mlogloss:1.08716\n",
      "[214]\ttrain-mlogloss:0.972179\teval-mlogloss:1.0871\n",
      "[215]\ttrain-mlogloss:0.971739\teval-mlogloss:1.08711\n",
      "[216]\ttrain-mlogloss:0.971215\teval-mlogloss:1.08703\n",
      "[217]\ttrain-mlogloss:0.970699\teval-mlogloss:1.08701\n",
      "[218]\ttrain-mlogloss:0.970113\teval-mlogloss:1.08698\n",
      "[219]\ttrain-mlogloss:0.969667\teval-mlogloss:1.08699\n",
      "[220]\ttrain-mlogloss:0.969062\teval-mlogloss:1.08691\n",
      "[221]\ttrain-mlogloss:0.968455\teval-mlogloss:1.08694\n",
      "[222]\ttrain-mlogloss:0.967938\teval-mlogloss:1.08693\n",
      "[223]\ttrain-mlogloss:0.967289\teval-mlogloss:1.08686\n",
      "[224]\ttrain-mlogloss:0.966631\teval-mlogloss:1.08686\n",
      "[225]\ttrain-mlogloss:0.966051\teval-mlogloss:1.08688\n",
      "[226]\ttrain-mlogloss:0.96555\teval-mlogloss:1.08686\n",
      "[227]\ttrain-mlogloss:0.965146\teval-mlogloss:1.0869\n",
      "[228]\ttrain-mlogloss:0.964696\teval-mlogloss:1.08693\n",
      "[229]\ttrain-mlogloss:0.964204\teval-mlogloss:1.08692\n",
      "[230]\ttrain-mlogloss:0.96364\teval-mlogloss:1.08693\n",
      "[231]\ttrain-mlogloss:0.963029\teval-mlogloss:1.08694\n",
      "[232]\ttrain-mlogloss:0.962381\teval-mlogloss:1.08687\n",
      "[233]\ttrain-mlogloss:0.961823\teval-mlogloss:1.08684\n",
      "[234]\ttrain-mlogloss:0.961335\teval-mlogloss:1.0868\n",
      "[235]\ttrain-mlogloss:0.960901\teval-mlogloss:1.08683\n",
      "[236]\ttrain-mlogloss:0.96029\teval-mlogloss:1.08675\n",
      "[237]\ttrain-mlogloss:0.95978\teval-mlogloss:1.08676\n",
      "[238]\ttrain-mlogloss:0.959281\teval-mlogloss:1.08676\n",
      "[239]\ttrain-mlogloss:0.958706\teval-mlogloss:1.08676\n",
      "[240]\ttrain-mlogloss:0.958232\teval-mlogloss:1.08679\n",
      "[241]\ttrain-mlogloss:0.957762\teval-mlogloss:1.08685\n",
      "[242]\ttrain-mlogloss:0.957325\teval-mlogloss:1.08686\n",
      "[243]\ttrain-mlogloss:0.956784\teval-mlogloss:1.08685\n",
      "[244]\ttrain-mlogloss:0.956344\teval-mlogloss:1.08686\n",
      "[245]\ttrain-mlogloss:0.95571\teval-mlogloss:1.08676\n",
      "[246]\ttrain-mlogloss:0.955271\teval-mlogloss:1.08677\n",
      "[247]\ttrain-mlogloss:0.954848\teval-mlogloss:1.08679\n",
      "[248]\ttrain-mlogloss:0.954363\teval-mlogloss:1.08682\n",
      "[249]\ttrain-mlogloss:0.95399\teval-mlogloss:1.08684\n",
      "[250]\ttrain-mlogloss:0.953607\teval-mlogloss:1.08686\n",
      "[251]\ttrain-mlogloss:0.953234\teval-mlogloss:1.08685\n",
      "[252]\ttrain-mlogloss:0.952879\teval-mlogloss:1.08686\n",
      "[253]\ttrain-mlogloss:0.952481\teval-mlogloss:1.08688\n",
      "[254]\ttrain-mlogloss:0.952048\teval-mlogloss:1.08687\n",
      "[255]\ttrain-mlogloss:0.951609\teval-mlogloss:1.08688\n",
      "[256]\ttrain-mlogloss:0.950988\teval-mlogloss:1.08685\n",
      "Stopping. Best iteration:\n",
      "[236]\ttrain-mlogloss:0.96029\teval-mlogloss:1.08675\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/irteam/.pyenv/versions/3.6.4/envs/venv/lib/python3.6/site-packages/ipykernel_launcher.py:172: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.851645186137\n",
      "Feature importance:\n",
      "('renta', 21414)\n",
      "('age', 19446)\n",
      "('antiguedad', 18461)\n",
      "('age_prev', 13619)\n",
      "('antiguedad_prev', 13288)\n",
      "('fecha_alta_month', 12497)\n",
      "('nomprov', 11770)\n",
      "('fecha_alta_year', 9653)\n",
      "('renta_prev', 8888)\n",
      "('canal_entrada', 8055)\n",
      "('nomprov_prev', 6245)\n",
      "('canal_entrada_prev', 4733)\n",
      "('fecha_alta_month_prev', 4181)\n",
      "('ind_recibo_ult1_prev', 3446)\n",
      "('fecha_alta_year_prev', 3221)\n",
      "('sexo', 3205)\n",
      "('ind_ecue_fin_ult1_prev', 3082)\n",
      "('ind_cco_fin_ult1_prev', 3012)\n",
      "('ind_cno_fin_ult1_prev', 2851)\n",
      "('segmento', 2313)\n",
      "('ind_tjcr_fin_ult1_prev', 2054)\n",
      "('ind_reca_fin_ult1_prev', 1986)\n",
      "('segmento_prev', 1964)\n",
      "('ind_nom_pens_ult1_prev', 1661)\n",
      "('tiprel_1mes', 1560)\n",
      "('ind_valo_fin_ult1_prev', 1546)\n",
      "('ind_dela_fin_ult1_prev', 1545)\n",
      "('ind_ctop_fin_ult1_prev', 1543)\n",
      "('sexo_prev', 1343)\n",
      "('ind_nomina_ult1_prev', 1311)\n",
      "('ind_actividad_cliente', 1296)\n",
      "('tiprel_1mes_prev', 1162)\n",
      "('ind_ctpp_fin_ult1_prev', 1123)\n",
      "('ind_fond_fin_ult1_prev', 1009)\n",
      "('ind_ctma_fin_ult1_prev', 878)\n",
      "('ind_actividad_cliente_prev', 870)\n",
      "('indext', 774)\n",
      "('ind_nuevo', 689)\n",
      "('ind_plan_fin_ult1_prev', 605)\n",
      "('ind_hip_fin_ult1_prev', 488)\n",
      "('ind_nuevo_prev', 425)\n",
      "('ind_deco_fin_ult1_prev', 390)\n",
      "('indrel_1mes', 380)\n",
      "('indext_prev', 368)\n",
      "('pais_residencia', 247)\n",
      "('indrel_1mes_prev', 175)\n",
      "('ind_empleado_prev', 166)\n",
      "('ind_viv_fin_ult1_prev', 160)\n",
      "('ind_deme_fin_ult1_prev', 108)\n",
      "('ind_empleado', 106)\n",
      "('pais_residencia_prev', 102)\n",
      "('ind_ctju_fin_ult1_prev', 66)\n",
      "('ind_pres_fin_ult1_prev', 62)\n",
      "('ind_cder_fin_ult1_prev', 55)\n",
      "('indfall_prev', 33)\n",
      "('indrel', 30)\n",
      "('indfall', 23)\n",
      "('conyuemp', 16)\n",
      "('indresi_prev', 10)\n",
      "('ult_fec_cli_1t_month', 9)\n",
      "('indresi', 8)\n",
      "('ult_fec_cli_1t_year', 2)\n",
      "# 691.4656 sec\n"
     ]
    }
   ],
   "source": [
    "## 모델 학습\n",
    "# 학습을 위하여 데이터를 훈련, 테스트용으로 분리한다.\n",
    "# 학습에는 2016-01-28 ~ 2016-04-28 데이터만 사용하고, 검증에는 2016-05-28 데이터를 사용한다.\n",
    "use_dates = ['2016-01-28', '2016-02-28', '2016-03-28', '2016-04-28', '2016-05-28']\n",
    "trn = df_trn[df_trn['fecha_dato'].isin(use_dates)]\n",
    "tst = df_trn[df_trn['fecha_dato'] == '2016-06-28']\n",
    "del df_trn\n",
    "\n",
    "# 훈련 데이터에서 신규 구매 건수만 추출한다.\n",
    "X = []\n",
    "Y = []\n",
    "for i, prod in enumerate(prods):\n",
    "    prev = prod + '_prev'\n",
    "    prX = trn[(trn[prod] == 1) & (trn[prev] == 0)]\n",
    "    prY = np.zeros(prX.shape[0], dtype=np.int8) + i\n",
    "    X.append(prX)\n",
    "    Y.append(prY)\n",
    "XY = pd.concat(X)\n",
    "Y = np.hstack(Y)\n",
    "XY['y'] = Y\n",
    "\n",
    "# 훈련, 검증 데이터로 분리한다. \n",
    "vld_date = '2016-05-28'\n",
    "XY_trn = XY[XY['fecha_dato'] != vld_date]\n",
    "XY_vld = XY[XY['fecha_dato'] == vld_date]\n",
    "\n",
    "# XGBoost 모델 parameter를 설정한다.\n",
    "param = {\n",
    "    'booster': 'gbtree',\n",
    "    'max_depth': 8,\n",
    "    'nthread': 24,\n",
    "    'num_class': len(prods),\n",
    "    'objective': 'multi:softprob',\n",
    "    'silent': 1,\n",
    "    'eval_metric': 'mlogloss',\n",
    "    'eta': 0.1,\n",
    "    'min_child_weight': 10,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'colsample_bylevel': 0.9,\n",
    "    'seed': 2018,\n",
    "    }\n",
    "\n",
    "# 훈련, 검증 데이터를 XGBoost 형태로 변환한다.\n",
    "X_trn = XY_trn.as_matrix(columns=features)\n",
    "Y_trn = XY_trn.as_matrix(columns=['y'])\n",
    "dtrn = xgb.DMatrix(X_trn, label=Y_trn, feature_names=features)\n",
    "\n",
    "X_vld = XY_vld.as_matrix(columns=features)\n",
    "Y_vld = XY_vld.as_matrix(columns=['y'])\n",
    "dvld = xgb.DMatrix(X_vld, label=Y_vld, feature_names=features)\n",
    "\n",
    "# XGBoost 모델을 훈련 데이터로 학습한다!\n",
    "watch_list = [(dtrn, 'train'), (dvld, 'eval')]\n",
    "model = xgb.train(param, dtrn, num_boost_round=1000, evals=watch_list, early_stopping_rounds=20)\n",
    "\n",
    "# 학습한 모델을 저장한다.\n",
    "# 모델 학습 과정에서 vld logloss 값을 최소화하는 parameter를 찾아야 한다.\n",
    "import pickle\n",
    "pickle.dump(model, open(\"model/xgb.baseline.pkl\", \"wb\"))\n",
    "best_ntree_limit = model.best_ntree_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAP@7 평가척도를 위한 준비작업이다.\n",
    "# 고객 식별 번호를 추출한다.\n",
    "vld = trn[trn['fecha_dato'] == vld_date]\n",
    "ncodpers_vld = vld.as_matrix(columns=['ncodpers'])\n",
    "# 검증 데이터에서 신규 구매를 구한다.\n",
    "for prod in prods:\n",
    "    prev = prod + '_prev'\n",
    "    padd = prod + '_add'\n",
    "    vld[padd] = vld[prod] - vld[prev]    \n",
    "add_vld = vld.as_matrix(columns=[prod + '_add' for prod in prods])\n",
    "add_vld_list = [list() for i in range(len(ncodpers_vld))]\n",
    "\n",
    "# 고객별 신규 구매 정답값을 add_vld_list에 저장하고, 총 count를 count_vld에 저장한다.\n",
    "count_vld = 0\n",
    "for ncodper in range(len(ncodpers_vld)):\n",
    "    for prod in range(len(prods)):\n",
    "        if add_vld[ncodper, prod] > 0:\n",
    "            add_vld_list[ncodper].append(prod)\n",
    "            count_vld += 1\n",
    "\n",
    "# 검증 데이터에서 얻을 수 있는 MAP@7 최고점을 미리 구한다. (0.042663)\n",
    "print(mapk(add_vld_list, add_vld_list, 7, 0.0))\n",
    "\n",
    "# 검증 데이터에 대한 예측값을 구한다.\n",
    "X_vld = vld.as_matrix(columns=features)\n",
    "Y_vld = vld.as_matrix(columns=['y'])\n",
    "dvld = xgb.DMatrix(X_vld, label=Y_vld, feature_names=features)\n",
    "preds_vld = model.predict(dvld, ntree_limit=best_ntree_limit)\n",
    "\n",
    "# 저번달에 보유한 제품은 신규 구매가 불가하기 때문에, 확률값에서 미리 1을 빼준다\n",
    "preds_vld = preds_vld - vld.as_matrix(columns=[prod + '_prev' for prod in prods])\n",
    "\n",
    "# 검증 데이터 예측 상위 7개를 추출한다.\n",
    "result_vld = []\n",
    "for ncodper, pred in zip(ncodpers_vld, preds_vld):\n",
    "    y_prods = [(y,p,ip) for y,p,ip in zip(pred, prods, range(len(prods)))]\n",
    "    y_prods = sorted(y_prods, key=lambda a: a[0], reverse=True)[:7]\n",
    "    result_vld.append([ip for y,p,ip in y_prods])\n",
    "    \n",
    "# 검증 데이터에서의 MAP@7 점수를 구한다. (0.036466)\n",
    "print(mapk(add_vld_list, result_vld, 7, 0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# XGBoost 모델을 전체 훈련 데이터로 재학습한다!\n",
    "X_all = XY.as_matrix(columns=features)\n",
    "Y_all = XY.as_matrix(columns=['y'])\n",
    "dall = xgb.DMatrix(X_all, label=Y_all, feature_names=features)\n",
    "watch_list = [(dall, 'train')]\n",
    "best_ntree_limit = int(best_ntree_limit * (len(XY_trn) + len(XY_vld)) / len(XY_trn))\n",
    "model = xgb.train(param, dall, num_boost_round=best_ntree_limit, evals=watch_list)\n",
    "\n",
    "# 변수 중요도를 출력해본다. 예상하던 변수가 상위로 올라와있는가?\n",
    "print(\"Feature importance:\")\n",
    "for kv in sorted([(k,v) for k,v in model.get_fscore().items()], key=lambda kv: kv[1], reverse=True):\n",
    "    print(kv)\n",
    "\n",
    "# 캐글 제출을 위하여 테스트 데이터에 대한 예측값을 구한다.\n",
    "X_tst = tst.as_matrix(columns=features)\n",
    "dtst = xgb.DMatrix(X_tst, feature_names=features)\n",
    "preds_tst = model.predict(dtst, ntree_limit=best_ntree_limit)\n",
    "ncodpers_tst = tst.as_matrix(columns=['ncodpers'])\n",
    "preds_tst = preds_tst - tst.as_matrix(columns=[prod + '_prev' for prod in prods])\n",
    "\n",
    "# 제출 파일을 생성한다.\n",
    "submit_file = open('output/xgb.baseline.2015-06-28', 'w')\n",
    "submit_file.write('ncodpers,added_products\\n')\n",
    "for ncodper, pred in zip(ncodpers_tst, preds_tst):\n",
    "    y_prods = [(y,p,ip) for y,p,ip in zip(pred, prods, range(len(prods)))]\n",
    "    y_prods = sorted(y_prods, key=lambda a: a[0], reverse=True)[:7]\n",
    "    y_prods = [p for y,p,ip in y_prods]\n",
    "    submit_file.write('{},{}\\n'.format(int(ncodper), ' '.join(y_prods)))\n",
    "    \n",
    "print('# {} sec'.format(round(time.time() - st, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- private leaderboard 0.025018\n",
    "- rank : 1001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
