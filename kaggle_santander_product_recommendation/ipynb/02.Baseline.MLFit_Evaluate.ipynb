{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn = pd.read_csv('data/baseline.feature_engineer.trn.csv')\n",
    "y = pd.read_csv('data/baseline.clean.y.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Cross validation..\n",
      "# Iter 1 / 5\n",
      "[0]\ttrain-mlogloss:2.39523\teval-mlogloss:2.38703\n",
      "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mlogloss hasn't improved in 10 rounds.\n",
      "[1]\ttrain-mlogloss:2.25494\teval-mlogloss:2.24478\n",
      "[2]\ttrain-mlogloss:2.16127\teval-mlogloss:2.15028\n",
      "[3]\ttrain-mlogloss:2.0936\teval-mlogloss:2.08212\n",
      "[4]\ttrain-mlogloss:2.04285\teval-mlogloss:2.03103\n",
      "[5]\ttrain-mlogloss:2.00399\teval-mlogloss:1.99158\n",
      "[6]\ttrain-mlogloss:1.97403\teval-mlogloss:1.96157\n",
      "[7]\ttrain-mlogloss:1.9502\teval-mlogloss:1.93736\n",
      "[8]\ttrain-mlogloss:1.93118\teval-mlogloss:1.91814\n",
      "[9]\ttrain-mlogloss:1.91563\teval-mlogloss:1.90289\n",
      "[10]\ttrain-mlogloss:1.90299\teval-mlogloss:1.89023\n",
      "[11]\ttrain-mlogloss:1.89265\teval-mlogloss:1.88013\n",
      "[12]\ttrain-mlogloss:1.8841\teval-mlogloss:1.87167\n",
      "[13]\ttrain-mlogloss:1.87699\teval-mlogloss:1.86485\n",
      "[14]\ttrain-mlogloss:1.87083\teval-mlogloss:1.85889\n",
      "[15]\ttrain-mlogloss:1.86576\teval-mlogloss:1.85399\n",
      "[16]\ttrain-mlogloss:1.86145\teval-mlogloss:1.8502\n",
      "[17]\ttrain-mlogloss:1.85781\teval-mlogloss:1.8464\n",
      "[18]\ttrain-mlogloss:1.8545\teval-mlogloss:1.84332\n",
      "[19]\ttrain-mlogloss:1.85176\teval-mlogloss:1.84117\n",
      "[20]\ttrain-mlogloss:1.84925\teval-mlogloss:1.83872\n",
      "[21]\ttrain-mlogloss:1.84705\teval-mlogloss:1.8368\n",
      "[22]\ttrain-mlogloss:1.84512\teval-mlogloss:1.83501\n",
      "[23]\ttrain-mlogloss:1.84345\teval-mlogloss:1.83355\n",
      "[24]\ttrain-mlogloss:1.84193\teval-mlogloss:1.83251\n",
      "[25]\ttrain-mlogloss:1.83998\teval-mlogloss:1.83073\n",
      "[26]\ttrain-mlogloss:1.83871\teval-mlogloss:1.8296\n",
      "[27]\ttrain-mlogloss:1.83744\teval-mlogloss:1.8287\n",
      "[28]\ttrain-mlogloss:1.83637\teval-mlogloss:1.82812\n",
      "[29]\ttrain-mlogloss:1.83532\teval-mlogloss:1.82761\n",
      "[30]\ttrain-mlogloss:1.83428\teval-mlogloss:1.82704\n",
      "[31]\ttrain-mlogloss:1.83344\teval-mlogloss:1.82642\n",
      "[32]\ttrain-mlogloss:1.83271\teval-mlogloss:1.82571\n",
      "[33]\ttrain-mlogloss:1.83182\teval-mlogloss:1.82536\n",
      "[34]\ttrain-mlogloss:1.83104\teval-mlogloss:1.82463\n",
      "[35]\ttrain-mlogloss:1.8303\teval-mlogloss:1.82437\n",
      "[36]\ttrain-mlogloss:1.82947\teval-mlogloss:1.82373\n",
      "[37]\ttrain-mlogloss:1.82874\teval-mlogloss:1.8235\n",
      "[38]\ttrain-mlogloss:1.82772\teval-mlogloss:1.8231\n",
      "[39]\ttrain-mlogloss:1.82707\teval-mlogloss:1.82284\n",
      "[40]\ttrain-mlogloss:1.82631\teval-mlogloss:1.82252\n",
      "[41]\ttrain-mlogloss:1.82564\teval-mlogloss:1.82218\n",
      "[42]\ttrain-mlogloss:1.82499\teval-mlogloss:1.82192\n",
      "[43]\ttrain-mlogloss:1.82421\teval-mlogloss:1.82159\n",
      "[44]\ttrain-mlogloss:1.8237\teval-mlogloss:1.82142\n",
      "[45]\ttrain-mlogloss:1.82301\teval-mlogloss:1.82106\n",
      "[46]\ttrain-mlogloss:1.8223\teval-mlogloss:1.82066\n",
      "[47]\ttrain-mlogloss:1.82168\teval-mlogloss:1.82063\n",
      "[48]\ttrain-mlogloss:1.82102\teval-mlogloss:1.82038\n",
      "[49]\ttrain-mlogloss:1.82052\teval-mlogloss:1.82031\n",
      "[50]\ttrain-mlogloss:1.82003\teval-mlogloss:1.81995\n",
      "[51]\ttrain-mlogloss:1.81954\teval-mlogloss:1.81954\n",
      "[52]\ttrain-mlogloss:1.81896\teval-mlogloss:1.81942\n",
      "[53]\ttrain-mlogloss:1.81849\teval-mlogloss:1.81927\n",
      "[54]\ttrain-mlogloss:1.81794\teval-mlogloss:1.81915\n",
      "[55]\ttrain-mlogloss:1.81728\teval-mlogloss:1.81888\n",
      "[56]\ttrain-mlogloss:1.81681\teval-mlogloss:1.81888\n",
      "[57]\ttrain-mlogloss:1.81616\teval-mlogloss:1.8188\n",
      "[58]\ttrain-mlogloss:1.8157\teval-mlogloss:1.81868\n",
      "[59]\ttrain-mlogloss:1.81527\teval-mlogloss:1.81877\n",
      "[60]\ttrain-mlogloss:1.81466\teval-mlogloss:1.81845\n",
      "[61]\ttrain-mlogloss:1.81401\teval-mlogloss:1.81842\n",
      "[62]\ttrain-mlogloss:1.81351\teval-mlogloss:1.81817\n",
      "[63]\ttrain-mlogloss:1.81305\teval-mlogloss:1.8182\n",
      "[64]\ttrain-mlogloss:1.8126\teval-mlogloss:1.81815\n",
      "[65]\ttrain-mlogloss:1.81208\teval-mlogloss:1.81795\n",
      "[66]\ttrain-mlogloss:1.81164\teval-mlogloss:1.81773\n",
      "[67]\ttrain-mlogloss:1.81116\teval-mlogloss:1.81763\n",
      "[68]\ttrain-mlogloss:1.81062\teval-mlogloss:1.81751\n",
      "[69]\ttrain-mlogloss:1.81006\teval-mlogloss:1.81736\n",
      "[70]\ttrain-mlogloss:1.80958\teval-mlogloss:1.81737\n",
      "[71]\ttrain-mlogloss:1.80911\teval-mlogloss:1.81746\n",
      "[72]\ttrain-mlogloss:1.80872\teval-mlogloss:1.81739\n",
      "[73]\ttrain-mlogloss:1.80827\teval-mlogloss:1.81742\n",
      "[74]\ttrain-mlogloss:1.80777\teval-mlogloss:1.81754\n",
      "[75]\ttrain-mlogloss:1.80737\teval-mlogloss:1.81748\n",
      "[76]\ttrain-mlogloss:1.80692\teval-mlogloss:1.81723\n",
      "[77]\ttrain-mlogloss:1.80633\teval-mlogloss:1.8173\n",
      "[78]\ttrain-mlogloss:1.80581\teval-mlogloss:1.81741\n",
      "[79]\ttrain-mlogloss:1.80538\teval-mlogloss:1.81748\n",
      "[80]\ttrain-mlogloss:1.80493\teval-mlogloss:1.81733\n",
      "[81]\ttrain-mlogloss:1.80455\teval-mlogloss:1.81752\n",
      "[82]\ttrain-mlogloss:1.80411\teval-mlogloss:1.81755\n",
      "[83]\ttrain-mlogloss:1.80375\teval-mlogloss:1.81749\n",
      "[84]\ttrain-mlogloss:1.80333\teval-mlogloss:1.81756\n",
      "[85]\ttrain-mlogloss:1.80289\teval-mlogloss:1.81752\n",
      "[86]\ttrain-mlogloss:1.80257\teval-mlogloss:1.81747\n",
      "Stopping. Best iteration:\n",
      "[76]\ttrain-mlogloss:1.80692\teval-mlogloss:1.81723\n",
      "\n",
      "# Iter 2 / 5\n",
      "[0]\ttrain-mlogloss:2.39439\teval-mlogloss:2.38956\n",
      "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mlogloss hasn't improved in 10 rounds.\n",
      "[1]\ttrain-mlogloss:2.2539\teval-mlogloss:2.24912\n",
      "[2]\ttrain-mlogloss:2.16011\teval-mlogloss:2.15592\n",
      "[3]\ttrain-mlogloss:2.09235\teval-mlogloss:2.08757\n",
      "[4]\ttrain-mlogloss:2.04178\teval-mlogloss:2.03656\n",
      "[5]\ttrain-mlogloss:2.00278\teval-mlogloss:1.99734\n",
      "[6]\ttrain-mlogloss:1.97256\teval-mlogloss:1.96695\n",
      "[7]\ttrain-mlogloss:1.94877\teval-mlogloss:1.94325\n",
      "[8]\ttrain-mlogloss:1.92964\teval-mlogloss:1.9246\n",
      "[9]\ttrain-mlogloss:1.91417\teval-mlogloss:1.90893\n",
      "[10]\ttrain-mlogloss:1.90156\teval-mlogloss:1.89677\n",
      "[11]\ttrain-mlogloss:1.89115\teval-mlogloss:1.88647\n",
      "[12]\ttrain-mlogloss:1.88266\teval-mlogloss:1.87833\n",
      "[13]\ttrain-mlogloss:1.8756\teval-mlogloss:1.8714\n",
      "[14]\ttrain-mlogloss:1.86973\teval-mlogloss:1.86593\n",
      "[15]\ttrain-mlogloss:1.86461\teval-mlogloss:1.86115\n",
      "[16]\ttrain-mlogloss:1.86028\teval-mlogloss:1.85651\n",
      "[17]\ttrain-mlogloss:1.8566\teval-mlogloss:1.85288\n",
      "[18]\ttrain-mlogloss:1.85353\teval-mlogloss:1.85075\n",
      "[19]\ttrain-mlogloss:1.85079\teval-mlogloss:1.84853\n",
      "[20]\ttrain-mlogloss:1.84841\teval-mlogloss:1.8461\n",
      "[21]\ttrain-mlogloss:1.84638\teval-mlogloss:1.84468\n",
      "[22]\ttrain-mlogloss:1.84439\teval-mlogloss:1.84326\n",
      "[23]\ttrain-mlogloss:1.8425\teval-mlogloss:1.84144\n",
      "[24]\ttrain-mlogloss:1.84082\teval-mlogloss:1.84016\n",
      "[25]\ttrain-mlogloss:1.83937\teval-mlogloss:1.83895\n",
      "[26]\ttrain-mlogloss:1.83802\teval-mlogloss:1.83804\n",
      "[27]\ttrain-mlogloss:1.83682\teval-mlogloss:1.8372\n",
      "[28]\ttrain-mlogloss:1.83558\teval-mlogloss:1.83671\n",
      "[29]\ttrain-mlogloss:1.83446\teval-mlogloss:1.83571\n",
      "[30]\ttrain-mlogloss:1.83345\teval-mlogloss:1.83496\n",
      "[31]\ttrain-mlogloss:1.83257\teval-mlogloss:1.83449\n",
      "[32]\ttrain-mlogloss:1.83153\teval-mlogloss:1.83384\n",
      "[33]\ttrain-mlogloss:1.83074\teval-mlogloss:1.8333\n",
      "[34]\ttrain-mlogloss:1.82989\teval-mlogloss:1.83254\n",
      "[35]\ttrain-mlogloss:1.82896\teval-mlogloss:1.83179\n",
      "[36]\ttrain-mlogloss:1.82824\teval-mlogloss:1.83163\n",
      "[37]\ttrain-mlogloss:1.82755\teval-mlogloss:1.83175\n",
      "[38]\ttrain-mlogloss:1.82691\teval-mlogloss:1.83144\n",
      "[39]\ttrain-mlogloss:1.82616\teval-mlogloss:1.83102\n",
      "[40]\ttrain-mlogloss:1.82558\teval-mlogloss:1.83077\n",
      "[41]\ttrain-mlogloss:1.82499\teval-mlogloss:1.83021\n",
      "[42]\ttrain-mlogloss:1.82438\teval-mlogloss:1.82994\n",
      "[43]\ttrain-mlogloss:1.82369\teval-mlogloss:1.82964\n",
      "[44]\ttrain-mlogloss:1.82288\teval-mlogloss:1.82981\n",
      "[45]\ttrain-mlogloss:1.82219\teval-mlogloss:1.82974\n",
      "[46]\ttrain-mlogloss:1.82147\teval-mlogloss:1.82972\n",
      "[47]\ttrain-mlogloss:1.82091\teval-mlogloss:1.82959\n",
      "[48]\ttrain-mlogloss:1.82021\teval-mlogloss:1.82963\n",
      "[49]\ttrain-mlogloss:1.81961\teval-mlogloss:1.82944\n",
      "[50]\ttrain-mlogloss:1.81899\teval-mlogloss:1.82942\n",
      "[51]\ttrain-mlogloss:1.81841\teval-mlogloss:1.82899\n",
      "[52]\ttrain-mlogloss:1.81785\teval-mlogloss:1.82915\n",
      "[53]\ttrain-mlogloss:1.81731\teval-mlogloss:1.8291\n",
      "[54]\ttrain-mlogloss:1.8166\teval-mlogloss:1.82869\n",
      "[55]\ttrain-mlogloss:1.81601\teval-mlogloss:1.82855\n",
      "[56]\ttrain-mlogloss:1.81554\teval-mlogloss:1.82823\n",
      "[57]\ttrain-mlogloss:1.81494\teval-mlogloss:1.82827\n",
      "[58]\ttrain-mlogloss:1.81443\teval-mlogloss:1.8283\n",
      "[59]\ttrain-mlogloss:1.81384\teval-mlogloss:1.82818\n",
      "[60]\ttrain-mlogloss:1.81343\teval-mlogloss:1.82826\n",
      "[61]\ttrain-mlogloss:1.81271\teval-mlogloss:1.82775\n",
      "[62]\ttrain-mlogloss:1.81227\teval-mlogloss:1.82767\n",
      "[63]\ttrain-mlogloss:1.81168\teval-mlogloss:1.82776\n",
      "[64]\ttrain-mlogloss:1.81117\teval-mlogloss:1.82768\n",
      "[65]\ttrain-mlogloss:1.81067\teval-mlogloss:1.8278\n",
      "[66]\ttrain-mlogloss:1.81011\teval-mlogloss:1.82753\n",
      "[67]\ttrain-mlogloss:1.80963\teval-mlogloss:1.82748\n",
      "[68]\ttrain-mlogloss:1.80909\teval-mlogloss:1.82738\n",
      "[69]\ttrain-mlogloss:1.80861\teval-mlogloss:1.82729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[70]\ttrain-mlogloss:1.80795\teval-mlogloss:1.82701\n",
      "[71]\ttrain-mlogloss:1.80754\teval-mlogloss:1.82707\n",
      "[72]\ttrain-mlogloss:1.80712\teval-mlogloss:1.82717\n",
      "[73]\ttrain-mlogloss:1.80675\teval-mlogloss:1.8271\n",
      "[74]\ttrain-mlogloss:1.80633\teval-mlogloss:1.8269\n",
      "[75]\ttrain-mlogloss:1.80588\teval-mlogloss:1.82695\n",
      "[76]\ttrain-mlogloss:1.80539\teval-mlogloss:1.82668\n",
      "[77]\ttrain-mlogloss:1.80498\teval-mlogloss:1.82657\n",
      "[78]\ttrain-mlogloss:1.80447\teval-mlogloss:1.82661\n",
      "[79]\ttrain-mlogloss:1.80411\teval-mlogloss:1.82675\n",
      "[80]\ttrain-mlogloss:1.80375\teval-mlogloss:1.8266\n",
      "[81]\ttrain-mlogloss:1.80339\teval-mlogloss:1.8268\n",
      "[82]\ttrain-mlogloss:1.80292\teval-mlogloss:1.82686\n",
      "[83]\ttrain-mlogloss:1.80243\teval-mlogloss:1.82682\n",
      "[84]\ttrain-mlogloss:1.80195\teval-mlogloss:1.82708\n",
      "[85]\ttrain-mlogloss:1.80155\teval-mlogloss:1.82714\n",
      "[86]\ttrain-mlogloss:1.8011\teval-mlogloss:1.82724\n",
      "[87]\ttrain-mlogloss:1.80074\teval-mlogloss:1.82715\n",
      "Stopping. Best iteration:\n",
      "[77]\ttrain-mlogloss:1.80498\teval-mlogloss:1.82657\n",
      "\n",
      "# Iter 3 / 5\n",
      "[0]\ttrain-mlogloss:2.39287\teval-mlogloss:2.39796\n",
      "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mlogloss hasn't improved in 10 rounds.\n",
      "[1]\ttrain-mlogloss:2.2524\teval-mlogloss:2.25987\n",
      "[2]\ttrain-mlogloss:2.15858\teval-mlogloss:2.16833\n",
      "[3]\ttrain-mlogloss:2.09064\teval-mlogloss:2.1017\n",
      "[4]\ttrain-mlogloss:2.04\teval-mlogloss:2.05166\n",
      "[5]\ttrain-mlogloss:2.00088\teval-mlogloss:2.01381\n",
      "[6]\ttrain-mlogloss:1.97067\teval-mlogloss:1.98423\n",
      "[7]\ttrain-mlogloss:1.9467\teval-mlogloss:1.96106\n",
      "[8]\ttrain-mlogloss:1.92766\teval-mlogloss:1.94254\n",
      "[9]\ttrain-mlogloss:1.91203\teval-mlogloss:1.92778\n",
      "[10]\ttrain-mlogloss:1.89942\teval-mlogloss:1.91592\n",
      "[11]\ttrain-mlogloss:1.88895\teval-mlogloss:1.90606\n",
      "[12]\ttrain-mlogloss:1.8805\teval-mlogloss:1.89845\n",
      "[13]\ttrain-mlogloss:1.87335\teval-mlogloss:1.89213\n",
      "[14]\ttrain-mlogloss:1.86727\teval-mlogloss:1.88658\n",
      "[15]\ttrain-mlogloss:1.86209\teval-mlogloss:1.88217\n",
      "[16]\ttrain-mlogloss:1.8578\teval-mlogloss:1.87846\n",
      "[17]\ttrain-mlogloss:1.85416\teval-mlogloss:1.87553\n",
      "[18]\ttrain-mlogloss:1.85096\teval-mlogloss:1.87276\n",
      "[19]\ttrain-mlogloss:1.84825\teval-mlogloss:1.87068\n",
      "[20]\ttrain-mlogloss:1.84591\teval-mlogloss:1.86926\n",
      "[21]\ttrain-mlogloss:1.8438\teval-mlogloss:1.86753\n",
      "[22]\ttrain-mlogloss:1.84175\teval-mlogloss:1.86599\n",
      "[23]\ttrain-mlogloss:1.84001\teval-mlogloss:1.86502\n",
      "[24]\ttrain-mlogloss:1.83855\teval-mlogloss:1.86428\n",
      "[25]\ttrain-mlogloss:1.83703\teval-mlogloss:1.86283\n",
      "[26]\ttrain-mlogloss:1.83576\teval-mlogloss:1.86221\n",
      "[27]\ttrain-mlogloss:1.8345\teval-mlogloss:1.86122\n",
      "[28]\ttrain-mlogloss:1.83337\teval-mlogloss:1.86093\n",
      "[29]\ttrain-mlogloss:1.83219\teval-mlogloss:1.86043\n",
      "[30]\ttrain-mlogloss:1.83118\teval-mlogloss:1.85961\n",
      "[31]\ttrain-mlogloss:1.83031\teval-mlogloss:1.85891\n",
      "[32]\ttrain-mlogloss:1.82945\teval-mlogloss:1.85847\n",
      "[33]\ttrain-mlogloss:1.82867\teval-mlogloss:1.85813\n",
      "[34]\ttrain-mlogloss:1.82786\teval-mlogloss:1.85797\n",
      "[35]\ttrain-mlogloss:1.82688\teval-mlogloss:1.85745\n",
      "[36]\ttrain-mlogloss:1.82612\teval-mlogloss:1.85712\n",
      "[37]\ttrain-mlogloss:1.82538\teval-mlogloss:1.85677\n",
      "[38]\ttrain-mlogloss:1.82461\teval-mlogloss:1.85628\n",
      "[39]\ttrain-mlogloss:1.82393\teval-mlogloss:1.85648\n",
      "[40]\ttrain-mlogloss:1.8232\teval-mlogloss:1.85638\n",
      "[41]\ttrain-mlogloss:1.82232\teval-mlogloss:1.85608\n",
      "[42]\ttrain-mlogloss:1.82161\teval-mlogloss:1.85613\n",
      "[43]\ttrain-mlogloss:1.82096\teval-mlogloss:1.85632\n",
      "[44]\ttrain-mlogloss:1.82043\teval-mlogloss:1.85619\n",
      "[45]\ttrain-mlogloss:1.81986\teval-mlogloss:1.85611\n",
      "[46]\ttrain-mlogloss:1.81913\teval-mlogloss:1.85569\n",
      "[47]\ttrain-mlogloss:1.81849\teval-mlogloss:1.85546\n",
      "[48]\ttrain-mlogloss:1.81787\teval-mlogloss:1.85535\n",
      "[49]\ttrain-mlogloss:1.81725\teval-mlogloss:1.85514\n",
      "[50]\ttrain-mlogloss:1.81651\teval-mlogloss:1.8548\n",
      "[51]\ttrain-mlogloss:1.81596\teval-mlogloss:1.85496\n",
      "[52]\ttrain-mlogloss:1.81523\teval-mlogloss:1.85475\n",
      "[53]\ttrain-mlogloss:1.8147\teval-mlogloss:1.85461\n",
      "[54]\ttrain-mlogloss:1.81404\teval-mlogloss:1.85455\n",
      "[55]\ttrain-mlogloss:1.81357\teval-mlogloss:1.85443\n",
      "[56]\ttrain-mlogloss:1.81292\teval-mlogloss:1.85446\n",
      "[57]\ttrain-mlogloss:1.8123\teval-mlogloss:1.85446\n",
      "[58]\ttrain-mlogloss:1.8118\teval-mlogloss:1.85425\n",
      "[59]\ttrain-mlogloss:1.81123\teval-mlogloss:1.85364\n",
      "[60]\ttrain-mlogloss:1.81055\teval-mlogloss:1.8534\n",
      "[61]\ttrain-mlogloss:1.81003\teval-mlogloss:1.85347\n",
      "[62]\ttrain-mlogloss:1.80965\teval-mlogloss:1.85334\n",
      "[63]\ttrain-mlogloss:1.80928\teval-mlogloss:1.8533\n",
      "[64]\ttrain-mlogloss:1.80879\teval-mlogloss:1.85361\n",
      "[65]\ttrain-mlogloss:1.80833\teval-mlogloss:1.85374\n",
      "[66]\ttrain-mlogloss:1.80791\teval-mlogloss:1.85381\n",
      "[67]\ttrain-mlogloss:1.8074\teval-mlogloss:1.85359\n",
      "[68]\ttrain-mlogloss:1.8069\teval-mlogloss:1.85369\n",
      "[69]\ttrain-mlogloss:1.80642\teval-mlogloss:1.85374\n",
      "[70]\ttrain-mlogloss:1.806\teval-mlogloss:1.85359\n",
      "[71]\ttrain-mlogloss:1.80552\teval-mlogloss:1.8536\n",
      "[72]\ttrain-mlogloss:1.80499\teval-mlogloss:1.85339\n",
      "[73]\ttrain-mlogloss:1.80444\teval-mlogloss:1.85308\n",
      "[74]\ttrain-mlogloss:1.80397\teval-mlogloss:1.85317\n",
      "[75]\ttrain-mlogloss:1.80352\teval-mlogloss:1.8532\n",
      "[76]\ttrain-mlogloss:1.80308\teval-mlogloss:1.85312\n",
      "[77]\ttrain-mlogloss:1.80265\teval-mlogloss:1.85318\n",
      "[78]\ttrain-mlogloss:1.80221\teval-mlogloss:1.85316\n",
      "[79]\ttrain-mlogloss:1.80182\teval-mlogloss:1.85325\n",
      "[80]\ttrain-mlogloss:1.80146\teval-mlogloss:1.85349\n",
      "[81]\ttrain-mlogloss:1.80105\teval-mlogloss:1.85366\n",
      "[82]\ttrain-mlogloss:1.80075\teval-mlogloss:1.85369\n",
      "[83]\ttrain-mlogloss:1.80036\teval-mlogloss:1.85366\n",
      "Stopping. Best iteration:\n",
      "[73]\ttrain-mlogloss:1.80444\teval-mlogloss:1.85308\n",
      "\n",
      "# Iter 4 / 5\n",
      "[0]\ttrain-mlogloss:2.39437\teval-mlogloss:2.39098\n",
      "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mlogloss hasn't improved in 10 rounds.\n",
      "[1]\ttrain-mlogloss:2.25421\teval-mlogloss:2.2498\n",
      "[2]\ttrain-mlogloss:2.15988\teval-mlogloss:2.1561\n",
      "[3]\ttrain-mlogloss:2.09244\teval-mlogloss:2.08853\n",
      "[4]\ttrain-mlogloss:2.04221\teval-mlogloss:2.03829\n",
      "[5]\ttrain-mlogloss:2.00315\teval-mlogloss:1.99936\n",
      "[6]\ttrain-mlogloss:1.97309\teval-mlogloss:1.96921\n",
      "[7]\ttrain-mlogloss:1.9494\teval-mlogloss:1.94583\n",
      "[8]\ttrain-mlogloss:1.93008\teval-mlogloss:1.9265\n",
      "[9]\ttrain-mlogloss:1.91446\teval-mlogloss:1.91093\n",
      "[10]\ttrain-mlogloss:1.90207\teval-mlogloss:1.89898\n",
      "[11]\ttrain-mlogloss:1.89156\teval-mlogloss:1.88901\n",
      "[12]\ttrain-mlogloss:1.8832\teval-mlogloss:1.88114\n",
      "[13]\ttrain-mlogloss:1.87599\teval-mlogloss:1.87451\n",
      "[14]\ttrain-mlogloss:1.87006\teval-mlogloss:1.86885\n",
      "[15]\ttrain-mlogloss:1.86506\teval-mlogloss:1.86437\n",
      "[16]\ttrain-mlogloss:1.86081\teval-mlogloss:1.86068\n",
      "[17]\ttrain-mlogloss:1.85693\teval-mlogloss:1.8569\n",
      "[18]\ttrain-mlogloss:1.85384\teval-mlogloss:1.85414\n",
      "[19]\ttrain-mlogloss:1.85083\teval-mlogloss:1.8518\n",
      "[20]\ttrain-mlogloss:1.84843\teval-mlogloss:1.84986\n",
      "[21]\ttrain-mlogloss:1.84636\teval-mlogloss:1.84841\n",
      "[22]\ttrain-mlogloss:1.84445\teval-mlogloss:1.84648\n",
      "[23]\ttrain-mlogloss:1.84262\teval-mlogloss:1.84527\n",
      "[24]\ttrain-mlogloss:1.84085\teval-mlogloss:1.84446\n",
      "[25]\ttrain-mlogloss:1.83929\teval-mlogloss:1.8435\n",
      "[26]\ttrain-mlogloss:1.83801\teval-mlogloss:1.8424\n",
      "[27]\ttrain-mlogloss:1.8367\teval-mlogloss:1.84145\n",
      "[28]\ttrain-mlogloss:1.8355\teval-mlogloss:1.84079\n",
      "[29]\ttrain-mlogloss:1.83443\teval-mlogloss:1.84028\n",
      "[30]\ttrain-mlogloss:1.83343\teval-mlogloss:1.83953\n",
      "[31]\ttrain-mlogloss:1.83244\teval-mlogloss:1.83909\n",
      "[32]\ttrain-mlogloss:1.83152\teval-mlogloss:1.83857\n",
      "[33]\ttrain-mlogloss:1.8306\teval-mlogloss:1.83849\n",
      "[34]\ttrain-mlogloss:1.82976\teval-mlogloss:1.83814\n",
      "[35]\ttrain-mlogloss:1.82883\teval-mlogloss:1.83749\n",
      "[36]\ttrain-mlogloss:1.82809\teval-mlogloss:1.8373\n",
      "[37]\ttrain-mlogloss:1.82739\teval-mlogloss:1.83736\n",
      "[38]\ttrain-mlogloss:1.82658\teval-mlogloss:1.83685\n",
      "[39]\ttrain-mlogloss:1.82569\teval-mlogloss:1.83656\n",
      "[40]\ttrain-mlogloss:1.82495\teval-mlogloss:1.83657\n",
      "[41]\ttrain-mlogloss:1.82422\teval-mlogloss:1.83613\n",
      "[42]\ttrain-mlogloss:1.82349\teval-mlogloss:1.83621\n",
      "[43]\ttrain-mlogloss:1.82287\teval-mlogloss:1.83619\n",
      "[44]\ttrain-mlogloss:1.82233\teval-mlogloss:1.83636\n",
      "[45]\ttrain-mlogloss:1.82168\teval-mlogloss:1.83636\n",
      "[46]\ttrain-mlogloss:1.82108\teval-mlogloss:1.83643\n",
      "[47]\ttrain-mlogloss:1.82042\teval-mlogloss:1.83641\n",
      "[48]\ttrain-mlogloss:1.81989\teval-mlogloss:1.83642\n",
      "[49]\ttrain-mlogloss:1.81926\teval-mlogloss:1.83631\n",
      "[50]\ttrain-mlogloss:1.81859\teval-mlogloss:1.83639\n",
      "[51]\ttrain-mlogloss:1.81797\teval-mlogloss:1.83627\n",
      "Stopping. Best iteration:\n",
      "[41]\ttrain-mlogloss:1.82422\teval-mlogloss:1.83613\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Iter 5 / 5\n",
      "[0]\ttrain-mlogloss:2.39409\teval-mlogloss:2.39826\n",
      "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mlogloss hasn't improved in 10 rounds.\n",
      "[1]\ttrain-mlogloss:2.25318\teval-mlogloss:2.25845\n",
      "[2]\ttrain-mlogloss:2.15938\teval-mlogloss:2.16523\n",
      "[3]\ttrain-mlogloss:2.09147\teval-mlogloss:2.09791\n",
      "[4]\ttrain-mlogloss:2.04047\teval-mlogloss:2.04814\n",
      "[5]\ttrain-mlogloss:2.00145\teval-mlogloss:2.00934\n",
      "[6]\ttrain-mlogloss:1.9713\teval-mlogloss:1.97978\n",
      "[7]\ttrain-mlogloss:1.94757\teval-mlogloss:1.9566\n",
      "[8]\ttrain-mlogloss:1.92823\teval-mlogloss:1.93813\n",
      "[9]\ttrain-mlogloss:1.91269\teval-mlogloss:1.92299\n",
      "[10]\ttrain-mlogloss:1.90021\teval-mlogloss:1.91128\n",
      "[11]\ttrain-mlogloss:1.8896\teval-mlogloss:1.9012\n",
      "[12]\ttrain-mlogloss:1.88116\teval-mlogloss:1.89315\n",
      "[13]\ttrain-mlogloss:1.87395\teval-mlogloss:1.88687\n",
      "[14]\ttrain-mlogloss:1.86794\teval-mlogloss:1.88149\n",
      "[15]\ttrain-mlogloss:1.86287\teval-mlogloss:1.87672\n",
      "[16]\ttrain-mlogloss:1.85837\teval-mlogloss:1.87307\n",
      "[17]\ttrain-mlogloss:1.85449\teval-mlogloss:1.86987\n",
      "[18]\ttrain-mlogloss:1.85129\teval-mlogloss:1.86724\n",
      "[19]\ttrain-mlogloss:1.84842\teval-mlogloss:1.86505\n",
      "[20]\ttrain-mlogloss:1.84585\teval-mlogloss:1.86322\n",
      "[21]\ttrain-mlogloss:1.84371\teval-mlogloss:1.8616\n",
      "[22]\ttrain-mlogloss:1.84175\teval-mlogloss:1.86043\n",
      "[23]\ttrain-mlogloss:1.84003\teval-mlogloss:1.85928\n",
      "[24]\ttrain-mlogloss:1.83829\teval-mlogloss:1.85816\n",
      "[25]\ttrain-mlogloss:1.83668\teval-mlogloss:1.85711\n",
      "[26]\ttrain-mlogloss:1.83538\teval-mlogloss:1.85627\n",
      "[27]\ttrain-mlogloss:1.83415\teval-mlogloss:1.85579\n",
      "[28]\ttrain-mlogloss:1.83288\teval-mlogloss:1.85471\n",
      "[29]\ttrain-mlogloss:1.83189\teval-mlogloss:1.85443\n",
      "[30]\ttrain-mlogloss:1.8309\teval-mlogloss:1.8542\n",
      "[31]\ttrain-mlogloss:1.83007\teval-mlogloss:1.8537\n",
      "[32]\ttrain-mlogloss:1.82902\teval-mlogloss:1.85355\n",
      "[33]\ttrain-mlogloss:1.82797\teval-mlogloss:1.85305\n",
      "[34]\ttrain-mlogloss:1.82721\teval-mlogloss:1.85246\n",
      "[35]\ttrain-mlogloss:1.82643\teval-mlogloss:1.85251\n",
      "[36]\ttrain-mlogloss:1.82564\teval-mlogloss:1.85238\n",
      "[37]\ttrain-mlogloss:1.82492\teval-mlogloss:1.8521\n",
      "[38]\ttrain-mlogloss:1.82425\teval-mlogloss:1.85211\n",
      "[39]\ttrain-mlogloss:1.82356\teval-mlogloss:1.8518\n",
      "[40]\ttrain-mlogloss:1.82282\teval-mlogloss:1.85172\n",
      "[41]\ttrain-mlogloss:1.82203\teval-mlogloss:1.85143\n",
      "[42]\ttrain-mlogloss:1.82134\teval-mlogloss:1.8514\n",
      "[43]\ttrain-mlogloss:1.82071\teval-mlogloss:1.85139\n",
      "[44]\ttrain-mlogloss:1.81994\teval-mlogloss:1.85098\n",
      "[45]\ttrain-mlogloss:1.81925\teval-mlogloss:1.85099\n",
      "[46]\ttrain-mlogloss:1.81855\teval-mlogloss:1.85089\n",
      "[47]\ttrain-mlogloss:1.81786\teval-mlogloss:1.85078\n",
      "[48]\ttrain-mlogloss:1.81724\teval-mlogloss:1.85065\n",
      "[49]\ttrain-mlogloss:1.8166\teval-mlogloss:1.85064\n",
      "[50]\ttrain-mlogloss:1.81618\teval-mlogloss:1.85055\n",
      "[51]\ttrain-mlogloss:1.81567\teval-mlogloss:1.85046\n",
      "[52]\ttrain-mlogloss:1.81516\teval-mlogloss:1.85063\n",
      "[53]\ttrain-mlogloss:1.81462\teval-mlogloss:1.85049\n",
      "[54]\ttrain-mlogloss:1.81399\teval-mlogloss:1.85026\n",
      "[55]\ttrain-mlogloss:1.81346\teval-mlogloss:1.8501\n",
      "[56]\ttrain-mlogloss:1.81297\teval-mlogloss:1.85011\n",
      "[57]\ttrain-mlogloss:1.81257\teval-mlogloss:1.85017\n",
      "[58]\ttrain-mlogloss:1.81207\teval-mlogloss:1.84996\n",
      "[59]\ttrain-mlogloss:1.81153\teval-mlogloss:1.8499\n",
      "[60]\ttrain-mlogloss:1.81103\teval-mlogloss:1.84989\n",
      "[61]\ttrain-mlogloss:1.81042\teval-mlogloss:1.8498\n",
      "[62]\ttrain-mlogloss:1.80976\teval-mlogloss:1.8497\n",
      "[63]\ttrain-mlogloss:1.80921\teval-mlogloss:1.84976\n",
      "[64]\ttrain-mlogloss:1.80871\teval-mlogloss:1.84949\n",
      "[65]\ttrain-mlogloss:1.80827\teval-mlogloss:1.84956\n",
      "[66]\ttrain-mlogloss:1.80782\teval-mlogloss:1.8495\n",
      "[67]\ttrain-mlogloss:1.80715\teval-mlogloss:1.84951\n",
      "[68]\ttrain-mlogloss:1.80667\teval-mlogloss:1.84954\n",
      "[69]\ttrain-mlogloss:1.80618\teval-mlogloss:1.84954\n",
      "[70]\ttrain-mlogloss:1.80567\teval-mlogloss:1.84953\n",
      "[71]\ttrain-mlogloss:1.80529\teval-mlogloss:1.84964\n",
      "[72]\ttrain-mlogloss:1.80484\teval-mlogloss:1.8492\n",
      "[73]\ttrain-mlogloss:1.80444\teval-mlogloss:1.84925\n",
      "[74]\ttrain-mlogloss:1.80404\teval-mlogloss:1.84952\n",
      "[75]\ttrain-mlogloss:1.8037\teval-mlogloss:1.84966\n",
      "[76]\ttrain-mlogloss:1.80325\teval-mlogloss:1.84981\n",
      "[77]\ttrain-mlogloss:1.80275\teval-mlogloss:1.84964\n",
      "[78]\ttrain-mlogloss:1.80236\teval-mlogloss:1.84959\n",
      "[79]\ttrain-mlogloss:1.80187\teval-mlogloss:1.84946\n",
      "[80]\ttrain-mlogloss:1.80143\teval-mlogloss:1.8496\n",
      "[81]\ttrain-mlogloss:1.80105\teval-mlogloss:1.84988\n",
      "[82]\ttrain-mlogloss:1.80061\teval-mlogloss:1.85002\n",
      "Stopping. Best iteration:\n",
      "[72]\ttrain-mlogloss:1.80484\teval-mlogloss:1.8492\n",
      "\n",
      "# TRN logloss: 1.8044499568226506\n",
      "# VLD logloss: 1.8369159741383196\n",
      "# Best Iters : 67.8\n",
      "# Refit and predict on test data..\n"
     ]
    }
   ],
   "source": [
    "print('# Cross validation..')\n",
    "\n",
    "# XGB Model Param\n",
    "num_round = 500\n",
    "early_stop = 10\n",
    "xgb_params = {\n",
    "    'booster': 'gbtree',\n",
    "\n",
    "    # model complexity\n",
    "    'max_depth': 2,  # higher, more complex\n",
    "    # 'gamma': 3,    # lower, more complex\n",
    "    # 'min_child_weight': 5, # lower, more complex\n",
    "\n",
    "    # regularization via random\n",
    "    # 'colsample_bylevel': 0.7,\n",
    "    # 'colsample_bytree': 1,\n",
    "    # 'subsample': 0.8,\n",
    "\n",
    "    # regulization\n",
    "    # 'reg_alpha': 2,\n",
    "    # 'reg_lambda': 3,\n",
    "\n",
    "    # 'learning_rate': 0.03,\n",
    "\n",
    "    # basic\n",
    "    'nthread': 4,\n",
    "    'num_class': 15,\n",
    "    'objective': 'multi:softprob',\n",
    "    'silent': 1,\n",
    "    'eval_metric': 'mlogloss',\n",
    "    'seed': 777,\n",
    "}\n",
    "\n",
    "trn_scores = []\n",
    "vld_scores = []\n",
    "best_iters = []\n",
    "n_splits = 5\n",
    "sss = StratifiedShuffleSplit(n_splits=n_splits, test_size=0.1, random_state=777)\n",
    "for i, (t_ind, v_ind) in enumerate(sss.split(trn, y)):\n",
    "    print('# Iter {} / {}'.format(i+1, n_splits))\n",
    "    x_trn = np.asarray(trn)[t_ind]\n",
    "    x_vld = np.asarray(trn)[v_ind]\n",
    "    y_trn = np.asarray(y)[t_ind]\n",
    "    y_vld = np.asarray(y)[v_ind]\n",
    "\n",
    "    dtrn = xgb.DMatrix(x_trn, label=y_trn)\n",
    "    dvld = xgb.DMatrix(x_vld, label=y_vld)\n",
    "    watch_list = [(dtrn, 'train'), (dvld, 'eval')]\n",
    "\n",
    "    # fit xgb\n",
    "    bst = xgb.train(xgb_params, dtrn, num_round, watch_list,\n",
    "                    early_stopping_rounds=early_stop, verbose_eval=True)\n",
    "\n",
    "    # eval _ trn\n",
    "    score = log_loss(y_trn, bst.predict(dtrn))\n",
    "    trn_scores.append(score)\n",
    "\n",
    "    # eval _ vld\n",
    "    score = log_loss(y_vld, bst.predict(dvld))\n",
    "    vld_scores.append(score)\n",
    "\n",
    "    # best iters\n",
    "    best_iters.append(bst.best_iteration)\n",
    "\n",
    "print('# TRN logloss: {}'.format(np.mean(trn_scores)))\n",
    "print('# VLD logloss: {}'.format(np.mean(vld_scores)))\n",
    "print('# Best Iters : {}'.format(np.mean(best_iters)))\n",
    "# TRN logloss : 1.8049220522722245\n",
    "# VLD logloss : 1.8451546928938647\n",
    "# Best Iters  : 63.6\n",
    "\n",
    "##################################################################################################################\n",
    "### Model Fit\n",
    "##################################################################################################################\n",
    "\n",
    "print('# Refit and predict on test data..')\n",
    "dtrn = xgb.DMatrix(trn, label=y)\n",
    "num_round = int(np.mean(best_iters) / 0.9)\n",
    "bst = xgb.train(xgb_params, dtrn, num_round, verbose_eval=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst.save_model('model/baseline.depth2.trn-1.80.dev-1.83.xgb')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
